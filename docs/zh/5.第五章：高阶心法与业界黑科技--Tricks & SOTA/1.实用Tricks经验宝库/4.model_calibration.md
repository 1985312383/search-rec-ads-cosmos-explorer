---
title: 模型校准：让预测概率回归真实
createTime: 2025/06/05 09:34:56

---

# 模型校准：让预测概率回归真实

> "AUC 衡量的是排序能力，校准衡量的是概率的诚实程度——两者缺一不可"

## 🎯 什么是校准，为什么重要

一个模型预测某用户点击某物品的概率为 0.3，如果我们收集所有被预测为 0.3 的样本，发现其中真正点击的比例也接近 30%，那么这个模型就是**校准良好**的。数学上，完美校准要求：

$$P(Y=1 \mid f(X)=p) = p, \quad \forall p \in [0,1]$$

AUC 只关心排序——预测值 0.8 是否排在 0.3 前面，不关心 0.8 和 0.3 本身是否准确。但在很多业务场景中，预测概率的**绝对值**同样重要：

- **广告竞价**：eCPM = pCTR × bid。如果 pCTR 系统性偏高 2 倍，广告主的实际 CPC 就会翻倍，导致 ROI 恶化、预算提前耗尽。
- **多目标融合**：最终排序分 = w₁·pCTR + w₂·pCVR + w₃·时长预估。如果各目标的预测值量纲不一致（CTR 模型偏高、CVR 模型偏低），融合权重就失去了意义。
- **库存预估**：根据预测的曝光→点击→转化漏斗来预估广告库存。校准偏差会直接导致库存预估失准。

### 深度模型为什么容易失准

现代深度学习模型普遍存在**过度自信**问题（Guo et al., ICML 2017）。主要原因：

- **模型容量过大**：过参数化的网络有能力将训练集的噪声也拟合进去，导致预测概率趋向 0 或 1 的极端值。
- **交叉熵损失的特性**：交叉熵在样本被正确分类后仍会继续推动概率向极端值移动——即使样本已经被正确排序，损失函数仍然"鼓励"模型更加自信。
- **训练数据与线上分布不匹配**：样本选择偏差（如只用曝光过的样本训练）、正负样本比例调整（如负采样）都会导致预测概率偏离真实分布。

## 📊 如何衡量校准质量

### 可靠性图（Reliability Diagram）

最直观的校准评估工具。将预测概率分成若干个桶（如 [0, 0.1), [0.1, 0.2), ...），计算每个桶内的平均预测概率和实际正例率，画在同一张图上。完美校准的模型，所有点都落在对角线 $y=x$ 上。

- 点在对角线**上方**：模型对该区间**欠自信**（预测偏低）
- 点在对角线**下方**：模型对该区间**过自信**（预测偏高）

### ECE 与 MCE

**期望校准误差（Expected Calibration Error）** 是最常用的校准指标，本质上是可靠性图中各桶偏差的加权平均：

$$\text{ECE} = \sum_{b=1}^{B} \frac{n_b}{N} \left| \bar{p}_b - \bar{y}_b \right|$$

其中 $\bar{p}_b$ 是第 $b$ 个桶的平均预测概率，$\bar{y}_b$ 是该桶的实际正例率，$n_b/N$ 是该桶的样本占比。ECE 越小，校准越好。

**最大校准误差（Maximum Calibration Error）** 则关注最差的那个桶：$\text{MCE} = \max_b |\bar{p}_b - \bar{y}_b|$。在风险敏感的场景（如广告出价），MCE 比 ECE 更值得关注——一个桶的严重偏差就可能造成大量资金浪费。

### Brier Score 分解

Brier Score 是预测概率与真实标签之间的均方误差：$\text{BS} = \frac{1}{N}\sum_i (p_i - y_i)^2$。它可以分解为三个有意义的分量：

$$\text{BS} = \underbrace{\text{Reliability}}_{\text{校准误差}} - \underbrace{\text{Resolution}}_{\text{区分能力}} + \underbrace{\text{Uncertainty}}_{\text{数据本身的不确定性}}$$

- **Reliability**（越小越好）：衡量校准质量，与 ECE 类似
- **Resolution**（越大越好）：衡量模型区分不同样本的能力
- **Uncertainty**（不可控）：由数据本身的正负比例决定

这个分解的价值在于：它帮助你判断模型的问题出在"排序不好"（Resolution 低）还是"概率不准"（Reliability 高），从而决定是该优化模型结构还是做后处理校准。

## 🔧 校准方法

校准方法分为两大类：**训练时校准**（在训练过程中改善校准）和**后处理校准**（在模型输出之后做映射修正）。工业界最常用的是后处理校准，因为它不需要重新训练模型，可以快速迭代。

### 后处理校准

#### Platt Scaling

用一个逻辑回归将模型的原始输出 $s$ 映射为校准后的概率：

$$p_{\text{cal}} = \sigma(a \cdot s + b)$$

在一个独立的验证集上拟合参数 $a$ 和 $b$（最大化对数似然）。Platt Scaling 只有两个参数，不容易过拟合，是最简单也最常用的校准方法。

局限性：它假设校准映射是 sigmoid 形状的，如果模型的失准模式更复杂（如在不同概率区间偏差方向不同），Platt Scaling 就力不从心了。

#### 保序回归（Isotonic Regression）

一种非参数方法：在验证集上学习一个**单调递增的分段常数函数**，将原始预测映射为校准概率。它不做任何分布假设，理论上可以修正任意形状的失准。

代价是：参数量与验证集的桶数成正比，在验证集较小时容易过拟合。实践中通常需要对桶数做交叉验证。

#### 温度缩放（Temperature Scaling）

对模型输出的 logit 除以一个温度参数 $T$：

$$p_{\text{cal}} = \sigma(s / T)$$

$T > 1$ 使概率分布更平滑（降低自信度），$T < 1$ 使分布更尖锐。温度缩放只有一个参数，在验证集上通过最小化 NLL 来优化。

温度缩放的一个重要性质是：它**不改变排序**（因为 $s/T$ 是 $s$ 的单调变换），只调整概率的绝对值。这意味着 AUC 不变，只有 ECE 改善。

#### 三种方法的选择

| 方法 | 参数量 | 过拟合风险 | 灵活性 | 适用场景 |
|------|--------|-----------|--------|---------|
| Platt Scaling | 2 | 低 | 中（假设 sigmoid 形状） | 验证集较小、失准模式简单 |
| 保序回归 | O(桶数) | 中 | 高（任意单调映射） | 验证集充足、失准模式复杂 |
| 温度缩放 | 1 | 极低 | 低（只缩放不变形） | 深度模型的过度自信问题 |

### 训练时校准

#### 标签平滑（Label Smoothing）

将硬标签 $y \in \{0, 1\}$ 替换为软标签 $y_{\text{smooth}} = (1-\epsilon) \cdot y + \epsilon / K$，其中 $\epsilon$ 通常取 0.1。这阻止模型将预测概率推向极端值，从而在训练阶段就改善校准。

#### Focal Loss

$$\text{FL}(p_t) = -\alpha (1-p_t)^\gamma \log(p_t)$$

Focal Loss 通过 $(1-p_t)^\gamma$ 因子降低"容易样本"（模型已经很自信的样本）的损失权重，使模型更关注"困难样本"。$\gamma=2$ 是最常用的设置。虽然 Focal Loss 的初衷是解决类别不平衡问题，但它对校准也有正面影响——因为它减少了模型在已经正确分类的样本上继续"加码"的动力。

### 推荐系统中的特殊考量

#### 分层校准

不同用户群体、不同物品品类的校准偏差可能截然不同。全局校准可能在某些子群体上反而恶化。分层校准的思路是：

1. 先做全局校准，建立基线
2. 按品类/用户群体/场景分组，在每个组内做局部校准
3. 对样本量不足的组，回退到全局校准（避免过拟合）

关键问题是分组粒度的选择：粒度太粗，无法捕捉组间差异；粒度太细，每组样本量不足。实践中通常按物品品类（几十到几百个组）做分层，用户侧则按活跃度分 3–5 档。

#### 时间漂移下的校准

模型的校准质量会随时间衰减——训练数据的分布与线上分布逐渐偏离。应对策略：

- **滑动窗口校准**：只用最近 N 天的数据拟合校准参数，定期（如每天）更新
- **在线校准**：将校准参数也纳入在线学习框架，随数据流实时更新
- **监控与告警**：持续监控线上 ECE，当超过阈值时触发校准参数的重新拟合

#### 负采样率修正

如果训练时对负样本做了降采样（如只保留 10% 的负样本），模型学到的概率会系统性偏高。需要在推理时做修正：

$$p_{\text{corrected}} = \frac{p}{p + (1-p)/w}$$

其中 $w$ 是负样本的采样率（如 0.1）。这个修正是精确的，不需要额外拟合参数。

## 📖 延伸阅读

- [On Calibration of Modern Neural Networks (Guo et al., ICML 2017)](https://arxiv.org/abs/1706.04599)：揭示深度模型过度自信问题的经典论文，提出温度缩放
- [Practical Lessons from Predicting Clicks on Ads at Facebook (He et al., ADKDD 2014)](https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)：Facebook 广告系统的校准实践，包括负采样率修正
- [netcal](https://github.com/fabiankueppers/calibration-framework)：专门的神经网络校准 Python 库
- [uncertainty-toolbox](https://github.com/uncertainty-toolbox/uncertainty-toolbox)：不确定性量化与校准评估工具箱

> 🧠 **思考题**
>
> 1. 温度缩放不改变排序，Platt Scaling 和保序回归则可能改变排序。在什么场景下，你希望校准方法改变排序？在什么场景下，你希望保持排序不变？
> 2. 在多目标模型（同时预测 CTR 和 CVR）中，两个目标的校准应该独立进行还是联合进行？如果 CTR 模型校准后概率整体下降，会对 CTCVR = pCTR × pCVR 产生什么影响？
> 3. 分层校准中，如果某个品类的样本量只有几百条，直接在该品类上拟合 Platt Scaling 可能过拟合。你会如何设计一个"借力"机制，让小品类也能获得合理的校准？
> 4. 在 A/B 测试中，如何设计实验来验证校准方法的有效性？仅看 ECE 的改善是否足够？

::: tip 🎉 章节小结
模型校准解决的是"预测概率的绝对值是否可信"的问题，与排序能力（AUC）互补。温度缩放、Platt Scaling、保序回归是三种最常用的后处理校准方法，各有适用场景。在推荐系统中，分层校准、时间漂移下的动态校准、以及负采样率修正是三个需要特别关注的实践问题。
:::
