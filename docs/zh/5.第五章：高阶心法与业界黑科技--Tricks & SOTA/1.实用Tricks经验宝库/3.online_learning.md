---
title: 在线学习：让推荐系统跟上世界的变化
createTime: 2025/06/05 09:34:56

---

# 在线学习：让推荐系统跟上世界的变化

> "离线模型是对历史的总结，在线学习是对当下的感知"

## 🎯 为什么需要在线学习

推荐系统面对的数据分布是**非平稳**的：用户兴趣会迁移，新物品不断涌入，外部事件（节假日、热点新闻、促销活动）随时改变行为模式。纯离线训练的模型，从数据采集到上线部署，中间存在不可避免的时间差——这段时间里，世界已经变了。

在线学习的核心价值在于**缩短"数据→模型→决策"的反馈回路**。它不是要取代离线训练，而是在离线模型的基础上，用实时到达的数据持续微调模型参数，使模型始终贴近当前的数据分布。

### 离线训练 vs. 在线学习

| 维度 | 离线训练 | 在线学习 |
|------|---------|---------|
| 更新频率 | 小时/天级 | 分钟/秒级 |
| 数据使用 | 全量历史数据 | 增量新数据 |
| 计算模式 | 集中式大批量 | 分散式小批量/单样本 |
| 对分布变化的响应 | 下次重训才能感知 | 实时感知并适应 |
| 稳定性 | 高（全量数据平滑噪声） | 较低（易受单样本噪声影响） |
| 典型延迟 | 数据到上线：6–24 小时 | 数据到生效：秒到分钟级 |

工业界的主流做法是**两者结合**：离线全量训练提供一个稳定的基线模型，在线学习在此基础上做增量更新。这就是所谓的 Lambda 架构思想在模型层面的体现。

## 🧠 在线学习的理论基础

### 遗憾最小化

在线学习的理论框架建立在**遗憾（Regret）** 的概念上。遗憾衡量的是：在线决策的累积损失，与事后看来最优的固定策略之间的差距：

$$Regret_T = \sum_{t=1}^{T} \ell_t(w_t) - \min_{w^*} \sum_{t=1}^{T} \ell_t(w^*)$$

一个好的在线学习算法，其遗憾应该是**亚线性**增长的（即 $Regret_T = o(T)$），这意味着平均每轮的额外损失趋近于零。对于凸损失函数，在线梯度下降可以达到 $O(\sqrt{T})$ 的遗憾界——这是一个很强的理论保证。

### 概念漂移

概念漂移（Concept Drift）是在线学习面临的核心挑战，指数据的底层分布 $P(y|x)$ 随时间发生变化。根据变化方式的不同，可以分为：

- **突变漂移**：分布在某个时间点突然切换。典型场景：平台改版、政策变化。模型需要快速"忘记"旧模式。
- **渐变漂移**：分布缓慢、持续地变化。典型场景：用户兴趣的自然迁移。模型需要平滑地跟踪变化。
- **周期漂移**：分布呈周期性波动。典型场景：工作日/周末的行为差异、季节性消费模式。模型需要学会"记住"周期模式。
- **重现漂移**：过去出现过的分布重新出现。典型场景：每年双十一的购物模式。

不同类型的漂移需要不同的应对策略。突变漂移需要快速遗忘（小窗口、高学习率），渐变漂移需要平滑跟踪（指数衰减权重），周期漂移则可能需要引入时间特征或维护多个子模型。

## 🔧 核心算法

### FTRL：工业界在线学习的事实标准

Follow-The-Regularized-Leader（FTRL）是 Google 在 2013 年提出的在线优化算法，至今仍是工业界大规模在线学习的首选。它的核心优势在于**稀疏性**：通过 L1 正则化，FTRL 能在训练过程中自动将不重要的特征权重置零，这对于动辄数十亿维稀疏特征的推荐系统至关重要。

FTRL 的更新规则：

$$w_{t+1,i} = \begin{cases}
0 & \text{if } |z_{t,i}| \leq \lambda_1 \\
-\dfrac{z_{t,i} - \text{sign}(z_{t,i})\lambda_1}{\lambda_2 + (\beta + \sqrt{n_{t,i}})/\alpha} & \text{otherwise}
\end{cases}$$

其中 $z_{t,i}$ 是梯度的累积量，$n_{t,i}$ 是梯度平方的累积量。当某个特征的累积梯度信号不够强（$|z_{t,i}| \leq \lambda_1$），权重直接被截断为零。

**为什么 FTRL 优于简单的 SGD + L1？** 直接对 SGD 加 L1 正则，由于每步更新的梯度方向不一定指向零点，权重很难精确为零，只能在零附近振荡。FTRL 通过在累积梯度层面做软阈值截断，真正实现了稀疏解。

**超参数选择**：$\alpha$ 控制学习率的整体大小（通常 0.01–1.0），$\beta$ 控制学习率的平滑程度（通常设为 1.0），$\lambda_1$ 控制稀疏程度（越大越稀疏，通常 0.01–1.0），$\lambda_2$ 提供额外的 L2 正则（通常 0.01–0.1）。

### 自适应学习率算法

在在线学习场景中，不同特征的更新频率差异巨大：高频特征（如"是否周末"）每个样本都会更新，低频特征（如某个长尾品类 ID）可能很久才出现一次。对所有特征使用相同的学习率是不合理的。

**AdaGrad** 的核心思想是为每个特征维护独立的学习率，更新频繁的特征学习率自动衰减，更新稀少的特征保持较大的学习率：

$$w_{t+1,i} = w_{t,i} - \frac{\eta}{\sqrt{\sum_{s=1}^{t} g_{s,i}^2 + \epsilon}} \cdot g_{t,i}$$

AdaGrad 的问题是学习率单调递减，在长期训练中可能过早"冻结"。**Adam** 通过引入指数移动平均（而非简单累加）来缓解这个问题，但在推荐系统的超大规模稀疏场景中，FTRL 仍然是更常见的选择。

### Bandit 算法：探索与利用的平衡

推荐系统天然面临**探索-利用困境**（Exploration-Exploitation Dilemma）：应该推荐已知用户喜欢的内容（利用），还是尝试推荐未知的内容以发现新的兴趣（探索）？

**Thompson Sampling** 是目前工业界最常用的 Bandit 算法。对于每个候选物品，维护其点击率的 Beta 后验分布 $\theta_i \sim \text{Beta}(\alpha_i + S_i, \beta_i + F_i)$，每次推荐时从后验分布中采样，选择采样值最高的物品。这种方法的优雅之处在于：不确定性高的物品（数据少）后验分布更宽，被采样到高值的概率更大，从而自然地实现了探索；随着数据积累，后验分布收窄，算法自动转向利用。

**LinUCB** 则适用于有上下文信息的场景。它假设物品的期望奖励是上下文特征的线性函数，并通过置信上界来平衡探索与利用：

$$a_t = \arg\max_a \left( x_t^T \hat{\theta}_a + \alpha \sqrt{x_t^T A_a^{-1} x_t} \right)$$

第一项是期望奖励的估计，第二项是不确定性的度量。$\alpha$ 越大，探索越激进。

## 🏗️ 工程架构

### Lambda 架构：批流一体

工业级在线学习系统通常采用 Lambda 架构的变体：

- **批处理层**：定期（如每天）用全量数据重新训练模型，提供稳定的基线。
- **流处理层**：用 Flink/Spark Streaming 等框架处理实时数据流，增量更新模型参数。
- **服务层**：合并批处理和流处理的结果，对外提供推荐服务。

关键的工程决策：

**更新粒度**：逐样本更新（sample-by-sample）延迟最低但噪声最大；小批量更新（mini-batch，如每 100–1000 个样本）在延迟和稳定性之间取得平衡，是最常见的选择。

**模型同步**：在线更新的模型参数需要同步到所有推理服务节点。常见方案是将参数增量写入消息队列（如 Kafka），各节点异步消费。参数同步的延迟通常在秒级，对推荐效果的影响可以忽略。

**回滚机制**：在线学习可能因为数据异常（如爬虫流量、日志错误）导致模型质量骤降。必须有自动化的质量监控和回滚机制——当在线指标（如 AUC、CTR）跌破阈值时，自动回退到上一个稳定版本。

### 概念漂移检测

在线学习系统需要感知数据分布的变化，以便调整学习策略。常用的检测方法：

**ADWIN（Adaptive Windowing）**：维护一个动态大小的滑动窗口，当窗口内前后两段数据的统计量（如均值）存在显著差异时，判定发生了漂移，并缩小窗口以丢弃过时数据。ADWIN 的优势是不需要预设窗口大小，能自适应地调整。

**性能监控法**：更实用的做法是直接监控模型的在线指标。当滑动窗口内的 AUC 或 logloss 出现持续恶化（而非随机波动），就触发告警。可以用 Page-Hinkley 检验来区分"随机波动"和"系统性漂移"。

### 延迟反馈问题

推荐系统中，正反馈（如购买）往往存在延迟——用户可能点击后几小时甚至几天才完成购买。如果在线学习只用即时反馈训练，会系统性地低估转化率。

常见的应对策略：
- **等待窗口**：等待固定时间（如 1 小时）后再将样本用于训练，但这牺牲了实时性。
- **重要性加权**：对延迟到达的正样本赋予更高权重，补偿早期被当作负样本的偏差。
- **多阶段标签**：先用点击信号做即时更新，转化信号到达后再做二次更新。

## 📖 延伸阅读

- [Ad Click Prediction: a View from the Trenches (McMahan et al., KDD 2013)](https://research.google/pubs/pub41159/)：Google 提出 FTRL 的经典论文，工业在线学习的奠基之作
- [Vowpal Wabbit](https://vowpalwabbit.org/)：高性能在线学习开源库，支持多种在线算法
- [River](https://riverml.xyz/)：现代化的 Python 在线机器学习库，API 设计优雅
- [A Survey on Concept Drift Adaptation (Gama et al., ACM Computing Surveys 2014)](https://dl.acm.org/doi/10.1145/2523813)：概念漂移领域的综述

> 🧠 **思考题**
>
> 1. 在线学习的学习率如何设置？过大会导致模型在噪声上过拟合，过小则无法及时跟踪分布变化。你会如何根据概念漂移的类型（突变 vs. 渐变）动态调整学习率？
> 2. FTRL 的 L1 正则化会将不活跃特征的权重置零。但如果一个特征只是暂时不活跃（如"圣诞节相关"特征在非节日期间），被置零后节日来临时需要重新学习。如何解决这个问题？
> 3. 在线学习系统中，如何设计一套完整的数据质量防线，防止异常数据（爬虫、日志错误、攻击流量）污染模型？
> 4. 对于同时优化 CTR 和 CVR 的多目标模型，两个目标的在线更新节奏不同（点击反馈即时，转化反馈延迟）。如何协调它们的在线学习过程？

::: tip 🎉 章节小结
在线学习的核心是缩短数据到决策的反馈回路。FTRL 凭借其稀疏性优势成为工业界大规模在线学习的事实标准；Bandit 算法（尤其是 Thompson Sampling）为探索-利用问题提供了理论优雅且实践有效的解决方案；Lambda 架构将离线训练的稳定性与在线学习的实时性结合在一起。而概念漂移检测和延迟反馈处理，则是在线学习系统从"能跑"到"可靠"的关键工程挑战。
:::
