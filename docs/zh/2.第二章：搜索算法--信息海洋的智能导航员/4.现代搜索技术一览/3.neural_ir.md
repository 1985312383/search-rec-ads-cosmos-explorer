---
title: 从神经检索到 LLM 排序：深度学习在搜索中的演进实战
createTime: 2025/06/05 09:34:56
---

## 🤖 神经检索与 LLM 排序：搜索的“强力引擎”

> **神经检索 (Neural IR)** 是利用深度学习模型（如 BERT, RoBERTa, LLMs）来捕获文本间的复杂语义关系。本节将从底层架构到工业界最前沿的 **RankGPT** 方案进行细化拆解。

---

## 🏗️ 核心架构深度对比

| 架构类型 | 交互方式 (Interaction) | 核心算法与模型 | 工业界位置 |
| :--- | :--- | :--- | :--- |
| **双塔 (Bi-Encoder)** | $score = cos(E_q, E_d)$ | BGE-Base, E5, OpenAI-3-Small | **召回层** (千万级数据，ms 级响应) |
| **后期交互 (ColBERT)** | MaxSim 操作：逐 token 计算得分并聚合 | ColBERT-v2, BGE-M3 (ColBERT 模式) | **粗排层** (兼顾泛化与词级精准) |
| **交叉编码器 (Cross-Encoder)** | $score = MLP(BERT(q, d))$ | BGE-Reranker, Cross-Encoder-base | **精排层** (处理 Top 50-100) |
| **LLM 排序器 (Ranker)** | 生成式预测或序列重排 | RankGPT (GPT-4o-mini/Llama-3) | **末级重排** (处理 Top 10-20) |

---

## 🏗️ 现代搜索的“重器”：LLM 作为排序器

### 1) Listwise 排序逻辑：RankGPT 深度拆解
传统精排只看单条文档的相关性（Pointwise），而 Listwise 允许 LLM 在对比中进行决策。

**具体的滑动窗口算法 (Sliding Window)**：
当候选集超过 LLM 上下文限制（如 Top 50 候选，窗口大小为 10）时：
1. **初始化**：从列表底部开始，选取最后 10 个文档。
2. **重排**：LLM 给出这 10 个文档的相对顺序。
3. **移动**：窗口向上滑动，将重排后最相关的文档（Top 1）保留在窗口中，再加入上方的 9 个新文档。
4. **循环**：直到窗口覆盖列表顶端，最终顶部的 Top 1 即为全列表全局最优。

### 2) 关键 Prompt 策略 (Instruction Engineering)
一个能让模型稳定输出的精排 Prompt 必须包含：
- **Role**: 定义为专家评测员。
- **Few-shot**: 提供 1-2 个正确排序的示例。
- **Output Constraint**: 强制要求输出 JSON 或特定的序列格式，避免长篇大论。

---

## ⚙️ 神经模型的训练与蒸馏

### 1) 难负样本挖掘 (Hard Negative Mining)
神经检索模型训练的关键不在于“正样本”，而在于“足够难的负样本”。
- **做法**：先用 BM25 检索出与 Query 高度重合但其实不相关的文档，作为训练的负样本。
- **效果**：迫使模型学会区分“长得像”和“意思对”的区别。

### 2) 知识蒸馏：让小模型继承大模型的能力
**蒸馏损失函数 (Loss Function)**：
通常使用 **KL 散度 (Kullback-Leibler Divergence)**。
$$Loss = KL(P_{teacher} || P_{student})$$
- 教师模型（Cross-Encoder）给出候选集的平滑分布分数。
- 学生模型（Bi-Encoder）通过调整 Embedding 空间，尽可能拟合这个概率分布。
- **价值**：实现在线推理时仅用双塔的速度，获得接近精排的 NDCG 提升。

---

## 📏 评估指标：不仅仅是 NDCG
- **MRR (Mean Reciprocal Rank)**：首个正确答案的位置，适合 FAQ 搜索。
- **NDCG@K**：考虑排序顺序的综合得分，工业界精排的标准。
- **Top-1 准确率**：对于 RAG 场景至关重要，因为 LLM 通常只看最前面的几个片段。

---

## 🤔 思考题

> 1. **Bi-Encoder** 和 **Cross-Encoder** 的本质区别在于 Attention 的发生阶段。为什么 Cross-Encoder 的精度更高？
> 2. 在 **RankGPT** 的滑动窗口算法中，如果窗口大小设置得太小（如 2），会发生什么问题？（提示：局部最优与全局最优）。

::: tip 🎉 章节小结
神经检索让搜索引擎从“查词”变成了“识意”。在实际工程中，我们通过**双塔模型召回**确保海量覆盖，通过**知识蒸馏**压榨模型性能，最后通过 **LLM 排序** 实现极致的精准。这是一场精度、延迟与成本之间的精密权衡。
:::
