---
title: Embedding çš„åŠ›é‡ï¼šä¸‡ç‰©çš†å¯è¡¨ç¤º
createTime: 2025/06/11 09:41:52

---

åœ¨æ¨èç³»ç»Ÿçš„æ·±åº¦å­¦ä¹ é©å‘½ä¸­ï¼ŒEmbeddingï¼ˆåµŒå…¥ï¼‰æŠ€æœ¯æ‰®æ¼”ç€åŸºçŸ³çš„è§’è‰²ã€‚å®ƒå°†ç¦»æ•£çš„ç¬¦å·ä¸–ç•Œè½¬æ¢ä¸ºè¿ç»­çš„å‘é‡ç©ºé—´ï¼Œè®©æœºå™¨èƒ½å¤Ÿç†è§£ç”¨æˆ·ã€ç‰©å“ä¹ƒè‡³æ•´ä¸ªä¸–ç•Œçš„è¯­ä¹‰å…³ç³»ã€‚ä»Word2Vecçš„æ¨ªç©ºå‡ºä¸–ï¼Œåˆ°Item2Vecçš„æ¨èåº”ç”¨ï¼ŒåµŒå…¥å­¦ä¹ å·²ç»æˆä¸ºç°ä»£æ¨èç³»ç»Ÿä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒæŠ€æœ¯ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦åµŒå…¥æŠ€æœ¯ï¼Ÿ ğŸ¤”

### ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§

**One-Hotç¼–ç çš„é—®é¢˜ï¼š**
- **ç»´åº¦ç¾éš¾**ï¼šç‰©å“IDä¸º100ä¸‡æ—¶ï¼Œæ¯ä¸ªå‘é‡é•¿åº¦100ä¸‡
- **ç¨€ç–æ€§**ï¼š99.9999%çš„ä½ç½®éƒ½æ˜¯0
- **æ— è¯­ä¹‰ä¿¡æ¯**ï¼šæ— æ³•è¡¨è¾¾ç‰©å“é—´çš„ç›¸ä¼¼æ€§

**ç¤ºä¾‹å¯¹æ¯”ï¼š**
```python
# One-Hotç¼–ç 
item_1 = [1, 0, 0, 0, ..., 0]  # é•¿åº¦=100ä¸‡
item_2 = [0, 1, 0, 0, ..., 0]  # å®Œå…¨æ­£äº¤ï¼Œæ— ç›¸ä¼¼æ€§

# Embeddingç¼–ç 
item_1 = [0.2, -0.1, 0.8, 0.3, ...]  # é•¿åº¦=256
item_2 = [0.3, -0.2, 0.7, 0.4, ...]  # å¯è®¡ç®—ç›¸ä¼¼åº¦
```

**åµŒå…¥çš„ä¼˜åŠ¿ï¼š**
- **ç¨ å¯†è¡¨ç¤º**ï¼š100ä¸‡ç‰©å“ç”¨256ç»´å‘é‡è¡¨ç¤º
- **è¯­ä¹‰ä¸°å¯Œ**ï¼šç›¸ä¼¼ç‰©å“åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘  
- **å¯è®¡ç®—**ï¼šæ”¯æŒå‘é‡è¿ç®—ï¼Œå¦‚ç›¸ä¼¼åº¦è®¡ç®—

**æ•°å­¦æœ¬è´¨**ï¼šåµŒå…¥æ˜¯ä¸€ä¸ªæ˜ å°„å‡½æ•° $\phi: \mathcal{X} \rightarrow \mathbb{R}^d$ï¼Œå°†é«˜ç»´ç¨€ç–ç©ºé—´æ˜ å°„åˆ°ä½ç»´ç¨ å¯†ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å…³ç³»ï¼š
$$\text{sim}(x_i, x_j) \approx \text{sim}(\phi(x_i), \phi(x_j))$$

## Word2Vecï¼šåµŒå…¥å­¦ä¹ çš„å¼€å±±ä¹‹ä½œ ğŸ“š

### æ ¸å¿ƒæ€æƒ³ï¼šåˆ†å¸ƒå¼å‡è®¾

**åŸºæœ¬åŸç†**ï¼š"ä½ å¯ä»¥é€šè¿‡ä¸€ä¸ªè¯çš„æœ‹å‹æ¥äº†è§£è¿™ä¸ªè¯"
- åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„è¯å…·æœ‰ç›¸ä¼¼çš„å«ä¹‰
- å°†è¿™ä¸ªæ€æƒ³ç”¨æ•°å­¦å»ºæ¨¡ï¼š$P(\text{context} | \text{word})$

### Skip-gramæ¨¡å‹è¯¦è§£

**è®­ç»ƒç›®æ ‡**ï¼šç»™å®šä¸­å¿ƒè¯ï¼Œé¢„æµ‹å‘¨å›´è¯
$$\mathcal{L} = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$

å…¶ä¸­ $T$ æ˜¯è¯­æ–™æ€»é•¿åº¦ï¼Œ$c$ æ˜¯çª—å£å¤§å°ã€‚

**æ¦‚ç‡å»ºæ¨¡**ï¼š
$$P(w_o | w_i) = \frac{\exp(\mathbf{v}_{w_o}^T \mathbf{v}_{w_i})}{\sum_{w=1}^{V} \exp(\mathbf{v}_w^T \mathbf{v}_{w_i})}$$

**å®Œæ•´å®ç°ï¼š**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
import numpy as np

class SkipGram(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        # ä¸­å¿ƒè¯åµŒå…¥
        self.center_embeddings = nn.Embedding(vocab_size, embed_dim)
        # ä¸Šä¸‹æ–‡è¯åµŒå…¥
        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)
        
        # åˆå§‹åŒ–æƒé‡
        nn.init.uniform_(self.center_embeddings.weight, -0.5/embed_dim, 0.5/embed_dim)
        nn.init.uniform_(self.context_embeddings.weight, -0.5/embed_dim, 0.5/embed_dim)
    
    def forward(self, center_words, context_words, negative_words):
        # è·å–åµŒå…¥å‘é‡
        center_embeds = self.center_embeddings(center_words)  # [batch, embed_dim]
        context_embeds = self.context_embeddings(context_words)  # [batch, embed_dim]
        neg_embeds = self.context_embeddings(negative_words)  # [batch, neg_num, embed_dim]
        
        # æ­£æ ·æœ¬å¾—åˆ†
        pos_score = torch.sum(center_embeds * context_embeds, dim=1)
        pos_loss = -F.logsigmoid(pos_score)
        
        # è´Ÿæ ·æœ¬å¾—åˆ†
        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()
        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)
        
        return torch.mean(pos_loss + neg_loss)

class Word2VecTrainer:
    def __init__(self, sentences, min_count=5, window_size=5, embed_dim=100):
        self.sentences = sentences
        self.window_size = window_size
        self.embed_dim = embed_dim
        
        # æ„å»ºè¯æ±‡è¡¨
        self.build_vocab(min_count)
        
        # å‡†å¤‡è´Ÿé‡‡æ ·è¡¨
        self.build_negative_sampling_table()
        
    def build_vocab(self, min_count):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_count = Counter()
        for sentence in self.sentences:
            word_count.update(sentence.split())
        
        # è¿‡æ»¤ä½é¢‘è¯
        self.vocab = {word: idx for idx, (word, count) in 
                     enumerate(word_count.items()) if count >= min_count}
        self.idx2word = {idx: word for word, idx in self.vocab.items()}
        self.vocab_size = len(self.vocab)
        
    def build_negative_sampling_table(self, table_size=1e8):
        """æ„å»ºè´Ÿé‡‡æ ·è¡¨"""
        # è®¡ç®—è¯é¢‘çš„3/4æ¬¡æ–¹
        word_counts = Counter()
        for sentence in self.sentences:
            for word in sentence.split():
                if word in self.vocab:
                    word_counts[word] += 1
        
        # æ„å»ºé‡‡æ ·è¡¨
        power = 0.75
        norm = sum([count**power for count in word_counts.values()])
        table = []
        
        for word, count in word_counts.items():
            prob = (count**power) / norm
            table.extend([self.vocab[word]] * int(prob * table_size))
        
        self.negative_table = np.array(table)
        
    def get_negative_samples(self, target_word, num_negatives=5):
        """è·å–è´Ÿæ ·æœ¬"""
        negatives = []
        while len(negatives) < num_negatives:
            neg_word = np.random.choice(self.negative_table)
            if neg_word != target_word:
                negatives.append(neg_word)
        return negatives
    
    def generate_training_data(self):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        training_data = []
        
        for sentence in self.sentences:
            words = [word for word in sentence.split() if word in self.vocab]
            
            for i, center_word in enumerate(words):
                center_idx = self.vocab[center_word]
                
                # è·å–ä¸Šä¸‹æ–‡çª—å£
                start = max(0, i - self.window_size)
                end = min(len(words), i + self.window_size + 1)
                
                for j in range(start, end):
                    if i != j:
                        context_word = words[j]
                        context_idx = self.vocab[context_word]
                        negatives = self.get_negative_samples(context_idx)
                        
                        training_data.append((center_idx, context_idx, negatives))
        
        return training_data

# è®­ç»ƒç¤ºä¾‹
def train_word2vec(sentences, epochs=10):
    trainer = Word2VecTrainer(sentences)
    model = SkipGram(trainer.vocab_size, trainer.embed_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    training_data = trainer.generate_training_data()
    
    for epoch in range(epochs):
        total_loss = 0
        
        # æ‰“ä¹±è®­ç»ƒæ•°æ®
        np.random.shuffle(training_data)
        
        for center, context, negatives in training_data:
            optimizer.zero_grad()
            
            center_tensor = torch.tensor([center])
            context_tensor = torch.tensor([context])
            neg_tensor = torch.tensor([negatives])
            
            loss = model(center_tensor, context_tensor, neg_tensor)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch {epoch+1}: Loss = {total_loss/len(training_data):.4f}')
    
    return model, trainer
```

### è´Ÿé‡‡æ ·ï¼šè®¡ç®—æ•ˆç‡çš„çªç ´

**é—®é¢˜**ï¼šSoftmaxè®¡ç®—å¤æ‚åº¦ä¸º $O(V)$ï¼Œè¯æ±‡è¡¨å¾ˆå¤§æ—¶è®¡ç®—æ˜‚è´µ

**è§£å†³æ–¹æ¡ˆ**ï¼šå°†å¤šåˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºäºŒåˆ†ç±»é—®é¢˜
- æ­£æ ·æœ¬ï¼šçœŸå®çš„ä¸Šä¸‹æ–‡è¯å¯¹
- è´Ÿæ ·æœ¬ï¼šéšæœºé‡‡æ ·çš„"å‡"ä¸Šä¸‹æ–‡è¯å¯¹

**è´Ÿé‡‡æ ·å…¬å¼**ï¼š
$$\log \sigma(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}_{w_i}^T \mathbf{v}_{w_I})]$$

**é‡‡æ ·åˆ†å¸ƒ**ï¼š$P_n(w) = \frac{U(w)^{3/4}}{Z}$ï¼Œå…¶ä¸­ $U(w)$ æ˜¯è¯é¢‘

**ä¸ºä»€ä¹ˆç”¨3/4æ¬¡æ–¹ï¼Ÿ**
- å¹³è¡¡é«˜é¢‘è¯å’Œä½é¢‘è¯çš„é‡‡æ ·æ¦‚ç‡
- é˜²æ­¢é«˜é¢‘è¯è¿‡åº¦é‡‡æ ·ï¼Œä½é¢‘è¯é‡‡æ ·ä¸è¶³

## Item2Vecï¼šä»æ–‡æœ¬åˆ°æ¨èçš„è¿ç§» ğŸ›ï¸

### æ ¸å¿ƒæ€æƒ³

**ç±»æ¯”è½¬æ¢ï¼š**
- **Word2Vec**ï¼šè¯ â†’ ä¸Šä¸‹æ–‡è¯
- **Item2Vec**ï¼šç‰©å“ â†’ ç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸­çš„ç›¸é‚»ç‰©å“

**åˆ†å¸ƒå¼å‡è®¾åœ¨æ¨èä¸­çš„åº”ç”¨ï¼š**
"åœ¨ç›¸ä¼¼ç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸­å‡ºç°çš„ç‰©å“å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§"

### ç®—æ³•æµç¨‹

**1. æ„å»ºç‰©å“åºåˆ—**
```python
def build_item_sequences(user_behaviors):
    """ä»ç”¨æˆ·è¡Œä¸ºæ„å»ºç‰©å“åºåˆ—"""
    sequences = []
    for user_id, items in user_behaviors.items():
        # æŒ‰æ—¶é—´æ’åº
        sorted_items = sorted(items, key=lambda x: x['timestamp'])
        item_sequence = [item['item_id'] for item in sorted_items]
        sequences.append(item_sequence)
    return sequences
```

**2. å®Œæ•´Item2Vecå®ç°**

```python
class Item2Vec:
    def __init__(self, sequences, embed_dim=128, window=5, min_count=5, 
                 negative=5, epochs=10, lr=0.001):
        self.sequences = sequences
        self.embed_dim = embed_dim
        self.window = window
        self.min_count = min_count
        self.negative = negative
        self.epochs = epochs
        self.lr = lr
        
        # æ„å»ºè¯æ±‡è¡¨
        self.build_vocab()
        # æ„å»ºè´Ÿé‡‡æ ·è¡¨
        self.build_negative_table()
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_model()
    
    def build_vocab(self):
        """æ„å»ºç‰©å“è¯æ±‡è¡¨"""
        item_count = Counter()
        for seq in self.sequences:
            item_count.update(seq)
        
        # è¿‡æ»¤ä½é¢‘ç‰©å“
        self.vocab = {}
        idx = 0
        for item, count in item_count.items():
            if count >= self.min_count:
                self.vocab[item] = idx
                idx += 1
        
        self.vocab_size = len(self.vocab)
        self.idx2item = {v: k for k, v in self.vocab.items()}
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
    
    def build_negative_table(self):
        """æ„å»ºè´Ÿé‡‡æ ·è¡¨"""
        item_count = Counter()
        for seq in self.sequences:
            for item in seq:
                if item in self.vocab:
                    item_count[item] += 1
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        power = 0.75
        total = sum([count**power for count in item_count.values()])
        
        table = []
        for item, count in item_count.items():
            if item in self.vocab:
                prob = (count**power) / total
                table.extend([self.vocab[item]] * int(prob * 1e8))
        
        self.negative_table = np.array(table)
    
    def init_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        self.center_embeddings = np.random.uniform(
            -0.5/self.embed_dim, 0.5/self.embed_dim, 
            (self.vocab_size, self.embed_dim)
        ).astype(np.float32)
        
        self.context_embeddings = np.random.uniform(
            -0.5/self.embed_dim, 0.5/self.embed_dim,
            (self.vocab_size, self.embed_dim)
        ).astype(np.float32)
    
    def get_negative_samples(self, target_idx, num_samples):
        """è·å–è´Ÿæ ·æœ¬"""
        negatives = []
        while len(negatives) < num_samples:
            neg_idx = np.random.choice(self.negative_table)
            if neg_idx != target_idx:
                negatives.append(neg_idx)
        return negatives
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        training_pairs = []
        
        for seq in self.sequences:
            # è¿‡æ»¤å‡ºè¯æ±‡è¡¨ä¸­çš„ç‰©å“
            seq_indices = [self.vocab[item] for item in seq if item in self.vocab]
            
            # ç”Ÿæˆè®­ç»ƒå¯¹
            for i, center_idx in enumerate(seq_indices):
                # å®šä¹‰çª—å£èŒƒå›´
                start = max(0, i - self.window)
                end = min(len(seq_indices), i + self.window + 1)
                
                for j in range(start, end):
                    if i != j:
                        context_idx = seq_indices[j]
                        negatives = self.get_negative_samples(context_idx, self.negative)
                        training_pairs.append((center_idx, context_idx, negatives))
        
        print(f"ç”Ÿæˆè®­ç»ƒæ ·æœ¬: {len(training_pairs)}")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.epochs):
            np.random.shuffle(training_pairs)
            total_loss = 0
            
            for center_idx, context_idx, negatives in training_pairs:
                # è®¡ç®—æ¢¯åº¦å’Œæ›´æ–°æƒé‡
                loss = self.train_pair(center_idx, context_idx, negatives)
                total_loss += loss
            
            avg_loss = total_loss / len(training_pairs)
            print(f"Epoch {epoch+1}: Loss = {avg_loss:.6f}")
    
    def train_pair(self, center_idx, context_idx, negatives):
        """è®­ç»ƒå•ä¸ªæ ·æœ¬å¯¹"""
        center_vec = self.center_embeddings[center_idx]
        context_vec = self.context_embeddings[context_idx]
        
        # æ­£æ ·æœ¬
        pos_score = np.dot(center_vec, context_vec)
        pos_prob = self.sigmoid(pos_score)
        
        # è®¡ç®—æ­£æ ·æœ¬æ¢¯åº¦
        pos_grad = (1 - pos_prob) * self.lr
        self.center_embeddings[center_idx] += pos_grad * context_vec
        self.context_embeddings[context_idx] += pos_grad * center_vec
        
        pos_loss = -np.log(pos_prob)
        
        # è´Ÿæ ·æœ¬
        neg_loss = 0
        for neg_idx in negatives:
            neg_vec = self.context_embeddings[neg_idx]
            neg_score = np.dot(center_vec, neg_vec)
            neg_prob = self.sigmoid(neg_score)
            
            # è®¡ç®—è´Ÿæ ·æœ¬æ¢¯åº¦
            neg_grad = -neg_prob * self.lr
            self.center_embeddings[center_idx] += neg_grad * neg_vec
            self.context_embeddings[neg_idx] += neg_grad * center_vec
            
            neg_loss += -np.log(1 - neg_prob)
        
        return pos_loss + neg_loss
    
    def sigmoid(self, x):
        """Sigmoidå‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def get_embedding(self, item):
        """è·å–ç‰©å“åµŒå…¥"""
        if item in self.vocab:
            idx = self.vocab[item]
            return self.center_embeddings[idx]
        return None
    
    def similarity(self, item1, item2):
        """è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦"""
        emb1 = self.get_embedding(item1)
        emb2 = self.get_embedding(item2)
        
        if emb1 is not None and emb2 is not None:
            # ä½™å¼¦ç›¸ä¼¼åº¦
            norm1 = np.linalg.norm(emb1)
            norm2 = np.linalg.norm(emb2)
            if norm1 > 0 and norm2 > 0:
                return np.dot(emb1, emb2) / (norm1 * norm2)
        return 0
    
    def most_similar(self, item, topk=10):
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç‰©å“"""
        if item not in self.vocab:
            return []
        
        target_emb = self.get_embedding(item)
        similarities = []
        
        for other_item in self.vocab:
            if other_item != item:
                sim = self.similarity(item, other_item)
                similarities.append((other_item, sim))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:topk]

# ä½¿ç”¨ç¤ºä¾‹
def train_item2vec_example():
    # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºæ•°æ®
    user_behaviors = {
        'user1': [
            {'item_id': 'item_1', 'timestamp': 1},
            {'item_id': 'item_2', 'timestamp': 2},
            {'item_id': 'item_3', 'timestamp': 3}
        ],
        'user2': [
            {'item_id': 'item_2', 'timestamp': 1},
            {'item_id': 'item_4', 'timestamp': 2},
            {'item_id': 'item_5', 'timestamp': 3}
        ]
        # æ›´å¤šç”¨æˆ·æ•°æ®...
    }
    
    # æ„å»ºåºåˆ—
    sequences = build_item_sequences(user_behaviors)
    
    # è®­ç»ƒItem2Vec
    model = Item2Vec(sequences, embed_dim=64, epochs=20)
    model.train()
    
    # æµ‹è¯•ç›¸ä¼¼åº¦
    similar_items = model.most_similar('item_1', topk=5)
    print(f"ä¸item_1æœ€ç›¸ä¼¼çš„ç‰©å“: {similar_items}")
    
    return model
```

### é«˜æ•ˆçš„Gensimå®ç°

```python
from gensim.models import Word2Vec

def train_item2vec_gensim(item_sequences, embed_dim=128, window=5, min_count=5):
    """ä½¿ç”¨Gensimè®­ç»ƒItem2Vecæ¨¡å‹"""
    # å°†item_idè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    str_sequences = [[str(item) for item in seq] for seq in item_sequences]
    
    model = Word2Vec(
        sentences=str_sequences,
        vector_size=embed_dim,    # åµŒå…¥ç»´åº¦
        window=window,            # çª—å£å¤§å°
        min_count=min_count,      # æœ€å°è¯é¢‘
        workers=4,                # å¹¶è¡Œçº¿ç¨‹æ•°
        sg=1,                     # ä½¿ç”¨Skip-gram
        negative=5,               # è´Ÿé‡‡æ ·æ•°é‡
        epochs=10,
        alpha=0.025,              # å­¦ä¹ ç‡
        min_alpha=0.0001          # æœ€å°å­¦ä¹ ç‡
    )
    
    return model

def use_trained_model(model):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # è·å–ç‰©å“åµŒå…¥
    item_embedding = model.wv['item_1']
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similar_items = model.wv.most_similar('item_1', topn=10)
    
    # ä¿å­˜æ¨¡å‹
    model.save('item2vec.model')
    
    # åŠ è½½æ¨¡å‹
    loaded_model = Word2Vec.load('item2vec.model')
    
    return similar_items
```

## åµŒå…¥ç©ºé—´çš„å‡ ä½•æ€§è´¨ ğŸ“

### å‘é‡è¿ç®—çš„è¯­ä¹‰è§£é‡Š

**å‘é‡åŠ æ³•çš„å‡ ä½•æ„ä¹‰**ï¼š
å¯¹äºç±»æ¯”å…³ç³» $a : b :: c : d$ï¼Œæœ‰ï¼š
$\mathbf{v}_b - \mathbf{v}_a \approx \mathbf{v}_d - \mathbf{v}_c$

è¿™ç­‰ä»·äºåœ¨åµŒå…¥ç©ºé—´ä¸­å¹³ç§»ä¸å˜æ€§ã€‚

**ç¤ºä¾‹éªŒè¯ï¼š**
```python
def test_analogy(model, word1, word2, word3):
    """æµ‹è¯•ç±»æ¯”å…³ç³» word1:word2 :: word3:?"""
    try:
        # è®¡ç®— word2 - word1 + word3
        result = model.wv.most_similar(
            positive=[word2, word3], 
            negative=[word1], 
            topn=1
        )
        return result[0][0]
    except:
        return None

# æµ‹è¯•ç¤ºä¾‹
# king - man + woman â‰ˆ queen
result = test_analogy(model, 'king', 'man', 'woman')
print(f"king - man + woman = {result}")
```

### åµŒå…¥ç»´åº¦çš„é€‰æ‹©

**ç»éªŒè§„å¾‹**ï¼š
å®è·µä¸­æœ€ä¼˜ç»´åº¦é€šå¸¸ä¸ºï¼š
$d^* \approx \alpha \sqrt[4]{V}$ï¼Œå…¶ä¸­ $\alpha \in [10, 30]$ï¼Œ$V$ æ˜¯è¯æ±‡è¡¨å¤§å°

**ç»´åº¦é€‰æ‹©å®éªŒï¼š**
```python
def dimension_experiment(sequences, dims=[32, 64, 128, 256]):
    """æµ‹è¯•ä¸åŒç»´åº¦çš„æ•ˆæœ"""
    results = {}
    
    for dim in dims:
        print(f"æµ‹è¯•ç»´åº¦: {dim}")
        
        # è®­ç»ƒæ¨¡å‹
        model = Item2Vec(sequences, embed_dim=dim, epochs=5)
        model.train()
        
        # è¯„ä¼°ç›¸ä¼¼åº¦è´¨é‡ï¼ˆéœ€è¦äººå·¥æ ‡æ³¨çš„ç›¸ä¼¼åº¦æ•°æ®ï¼‰
        # è¿™é‡Œç®€åŒ–ä¸ºè®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        avg_sim = evaluate_similarity_quality(model)
        results[dim] = avg_sim
        
        print(f"ç»´åº¦ {dim}: å¹³å‡ç›¸ä¼¼åº¦ = {avg_sim:.4f}")
    
    return results

def evaluate_similarity_quality(model):
    """è¯„ä¼°ç›¸ä¼¼åº¦è´¨é‡"""
    # ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼šè®¡ç®—æ‰€æœ‰ç‰©å“å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
    similarities = []
    items = list(model.vocab.keys())[:100]  # å–å‰100ä¸ªç‰©å“
    
    for i in range(len(items)):
        for j in range(i+1, len(items)):
            sim = model.similarity(items[i], items[j])
            similarities.append(sim)
    
    return np.mean(similarities)
```

## å®é™…åº”ç”¨ä¸­çš„æŠ€å·§ ğŸ”§

### å†·å¯åŠ¨é—®é¢˜å¤„ç†

```python
class HybridItem2Vec:
    def __init__(self, item_features=None):
        self.item_features = item_features  # ç‰©å“å†…å®¹ç‰¹å¾
        self.behavior_model = None          # è¡Œä¸ºåºåˆ—æ¨¡å‹
        self.content_model = None           # å†…å®¹ç‰¹å¾æ¨¡å‹
    
    def train_behavior_model(self, sequences):
        """è®­ç»ƒåŸºäºè¡Œä¸ºçš„æ¨¡å‹"""
        self.behavior_model = Item2Vec(sequences)
        self.behavior_model.train()
    
    def train_content_model(self, item_features):
        """è®­ç»ƒåŸºäºå†…å®¹çš„æ¨¡å‹"""
        # ä½¿ç”¨ç‰©å“ç‰¹å¾è®­ç»ƒåµŒå…¥
        # è¿™é‡Œç®€åŒ–å®ç°
        pass
    
    def get_embedding(self, item):
        """è·å–ç‰©å“åµŒå…¥ï¼Œå¤„ç†å†·å¯åŠ¨"""
        # é¦–å…ˆå°è¯•ä»è¡Œä¸ºæ¨¡å‹è·å–
        if self.behavior_model and item in self.behavior_model.vocab:
            return self.behavior_model.get_embedding(item)
        
        # å¦‚æœæ˜¯æ–°ç‰©å“ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾
        elif self.content_model and item in self.item_features:
            return self.content_model.get_embedding(item)
        
        # éƒ½æ²¡æœ‰åˆ™è¿”å›éšæœºåµŒå…¥æˆ–å¹³å‡åµŒå…¥
        else:
            return np.random.normal(0, 0.1, self.behavior_model.embed_dim)
```

### å¢é‡è®­ç»ƒ

```python
class IncrementalItem2Vec:
    def __init__(self, model_path=None):
        if model_path:
            self.model = Word2Vec.load(model_path)
        else:
            self.model = None
    
    def update_with_new_sequences(self, new_sequences):
        """ä½¿ç”¨æ–°åºåˆ—å¢é‡æ›´æ–°æ¨¡å‹"""
        if self.model is None:
            # é¦–æ¬¡è®­ç»ƒ
            self.model = Word2Vec(
                new_sequences, vector_size=128, window=5, 
                min_count=5, workers=4, sg=1
            )
        else:
            # å¢é‡è®­ç»ƒ
            self.model.build_vocab(new_sequences, update=True)
            self.model.train(new_sequences, total_examples=len(new_sequences), epochs=5)
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save(path)
```

### å¤šæ¨¡æ€åµŒå…¥

```python
class MultiModalEmbedding:
    def __init__(self, embed_dim=128):
        self.embed_dim = embed_dim
        self.behavior_embeds = {}    # è¡Œä¸ºåµŒå…¥
        self.content_embeds = {}     # å†…å®¹åµŒå…¥
        self.fusion_weights = None   # èåˆæƒé‡
    
    def train_behavior_embedding(self, sequences):
        """è®­ç»ƒè¡Œä¸ºåµŒå…¥"""
        model = Item2Vec(sequences, embed_dim=self.embed_dim)
        model.train()
        
        for item in model.vocab:
            self.behavior_embeds[item] = model.get_embedding(item)
    
    def train_content_embedding(self, item_features):
        """è®­ç»ƒå†…å®¹åµŒå…¥"""
        # ä½¿ç”¨ç‰©å“ç‰¹å¾è®­ç»ƒå†…å®¹åµŒå…¥
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨å„ç§æ–¹æ³•ï¼šTF-IDF + é™ç»´ï¼Œé¢„è®­ç»ƒæ¨¡å‹ç­‰
        pass
    
    def fuse_embeddings(self, item, alpha=0.7):
        """èåˆå¤šç§åµŒå…¥"""
        behavior_emb = self.behavior_embeds.get(item)
        content_emb = self.content_embeds.get(item)
        
        if behavior_emb is not None and content_emb is not None:
            # åŠ æƒå¹³å‡
            return alpha * behavior_emb + (1 - alpha) * content_emb
        elif behavior_emb is not None:
            return behavior_emb
        elif content_emb is not None:
            return content_emb
        else:
            return np.zeros(self.embed_dim)
```

## è¯„ä¼°ä¸ä¼˜åŒ– ğŸ“Š

### åµŒå…¥è´¨é‡è¯„ä¼°

```python
def evaluate_embedding_quality(model, test_similarities=None):
    """è¯„ä¼°åµŒå…¥è´¨é‡"""
    metrics = {}
    
    # 1. å†…åœ¨è¯„ä¼°ï¼šè¯ç±»æ¯”ä»»åŠ¡
    if hasattr(model, 'evaluate_word_analogies'):
        analogy_score = model.evaluate_word_analogies('analogies.txt')
        metrics['analogy_accuracy'] = analogy_score
    
    # 2. å¤–åœ¨è¯„ä¼°ï¼šæ¨èä»»åŠ¡æ€§èƒ½
    if test_similarities:
        pred_sims = []
        true_sims = []
        
        for (item1, item2), true_sim in test_similarities.items():
            pred_sim = model.similarity(item1, item2)
            pred_sims.append(pred_sim)
            true_sims.append(true_sim)
        
        from scipy.stats import spearmanr
        correlation, p_value = spearmanr(pred_sims, true_sims)
        metrics['similarity_correlation'] = correlation
    
    # 3. åµŒå…¥ç©ºé—´åˆ†æ
    embeddings = np.array([model.get_embedding(item) for item in model.vocab if model.get_embedding(item) is not None])
    
    # è®¡ç®—æœ‰æ•ˆç»´åº¦
    pca = PCA()
    pca.fit(embeddings)
    explained_var_ratio = pca.explained_variance_ratio_
    
    # 90%æ–¹å·®å¯¹åº”çš„ç»´åº¦
    cumsum_ratio = np.cumsum(explained_var_ratio)
    effective_dim = np.argmax(cumsum_ratio >= 0.9) + 1
    metrics['effective_dimension'] = effective_dim
    
    return metrics

# å¯è§†åŒ–åµŒå…¥
def visualize_embeddings(model, items=None, method='tsne'):
    """å¯è§†åŒ–åµŒå…¥å‘é‡"""
    if items is None:
        items = list(model.vocab.keys())[:100]  # å–å‰100ä¸ª
    
    embeddings = np.array([model.get_embedding(item) for item in items])
    
    if method == 'tsne':
        from sklearn.manifold import TSNE
        embeddings_2d = TSNE(n_components=2, random_state=42).fit_transform(embeddings)
    elif method == 'pca':
        from sklearn.decomposition import PCA
        embeddings_2d = PCA(n_components=2).fit_transform(embeddings)
    
    # ç»˜å›¾
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 8))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)
    
    # æ ‡æ³¨éƒ¨åˆ†ç‰©å“
    for i in range(min(20, len(items))):
        plt.annotate(items[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]))
    
    plt.title(f'Item Embeddings Visualization ({method.upper()})')
    plt.show()
```

## å®è·µæ€»ç»“ ğŸ¯

### æœ€ä½³å®è·µ

1. **æ•°æ®é¢„å¤„ç†**ï¼š
   - è¿‡æ»¤ä½é¢‘ç‰©å“ï¼ˆmin_countå‚æ•°ï¼‰
   - åºåˆ—é•¿åº¦æˆªæ–­ï¼ˆé¿å…è¿‡é•¿åºåˆ—ï¼‰
   - æ—¶é—´çª—å£å†…çš„è¡Œä¸ºèšåˆ

2. **å‚æ•°è°ƒä¼˜**ï¼š
   - åµŒå…¥ç»´åº¦ï¼š$\sqrt[4]{\text{vocab_size}} \times [10, 30]$
   - çª—å£å¤§å°ï¼š3-10ï¼Œæ ¹æ®ä¸šåŠ¡åœºæ™¯è°ƒæ•´
   - è´Ÿé‡‡æ ·æ•°é‡ï¼š5-20

3. **å·¥ç¨‹ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æˆç†Ÿåº“ï¼ˆGensim, FastTextï¼‰
   - å¢é‡è®­ç»ƒæ”¯æŒ
   - å¤šæ¨¡æ€ä¿¡æ¯èåˆ

### å¸¸è§é—®é¢˜è§£å†³

1. **è®­ç»ƒä¸æ”¶æ•›**ï¼š
   - é™ä½å­¦ä¹ ç‡
   - å¢åŠ è®­ç»ƒè½®æ•°
   - æ£€æŸ¥æ•°æ®è´¨é‡

2. **ç›¸ä¼¼åº¦æ•ˆæœå·®**ï¼š
   - å¢åŠ è®­ç»ƒæ•°æ®
   - è°ƒæ•´çª—å£å¤§å°
   - å°è¯•ä¸åŒçš„è´Ÿé‡‡æ ·ç­–ç•¥

3. **å†·å¯åŠ¨é—®é¢˜**ï¼š
   - ç»“åˆå†…å®¹ç‰¹å¾
   - ä½¿ç”¨ç‰©å“åˆ†ç±»ä¿¡æ¯
   - é›†æˆå¤šç§åµŒå…¥æ–¹æ³•

> ğŸ’¡ **å…³é”®æ´å¯Ÿ**
> 
> Embeddingçš„æˆåŠŸåœ¨äºï¼š
> - åˆ†å¸ƒå¼å‡è®¾çš„åˆç†æ€§ï¼šç›¸ä¼¼ä¸Šä¸‹æ–‡ â†’ ç›¸ä¼¼è¯­ä¹‰
> - è´Ÿé‡‡æ ·çš„è®¡ç®—æ•ˆç‡ï¼šä»O(V)é™ä½åˆ°O(k)
> - å‘é‡ç©ºé—´çš„å‡ ä½•æ€§è´¨ï¼šæ”¯æŒè¯­ä¹‰è¿ç®—
> - è¿ç§»å­¦ä¹ çš„å¯èƒ½æ€§ï¼šWord2Vec â†’ Item2Vec

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥æŒæ¡äº†EmbeddingæŠ€æœ¯çš„æ ¸å¿ƒåŸç†å’Œå®ç°æ–¹æ³•ã€‚è¿™ä¸ºç†è§£æ›´å¤æ‚çš„æ·±åº¦æ¨èæ¨¡å‹å¥ å®šäº†åšå®åŸºç¡€ã€‚
