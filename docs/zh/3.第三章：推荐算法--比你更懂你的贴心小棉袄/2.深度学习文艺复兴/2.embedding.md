---
title: Embedding 的力量：万物皆可表示
createTime: 2025/06/11 09:41:52

---

在推荐系统的深度学习革命中，Embedding（嵌入）技术扮演着基石的角色。它将离散的符号世界转换为连续的向量空间，让机器能够理解用户、物品乃至整个世界的语义关系。从Word2Vec的横空出世，到Item2Vec的推荐应用，嵌入学习已经成为现代推荐系统不可或缺的核心技术。

## 为什么需要嵌入技术？ 🤔

### 传统方法的局限性

**One-Hot编码的问题：**
- **维度灾难**：物品ID为100万时，每个向量长度100万
- **稀疏性**：99.9999%的位置都是0
- **无语义信息**：无法表达物品间的相似性

**示例对比：**
```python
# One-Hot编码
item_1 = [1, 0, 0, 0, ..., 0]  # 长度=100万
item_2 = [0, 1, 0, 0, ..., 0]  # 完全正交，无相似性

# Embedding编码
item_1 = [0.2, -0.1, 0.8, 0.3, ...]  # 长度=256
item_2 = [0.3, -0.2, 0.7, 0.4, ...]  # 可计算相似度
```

**嵌入的优势：**
- **稠密表示**：100万物品用256维向量表示
- **语义丰富**：相似物品在向量空间中距离较近  
- **可计算**：支持向量运算，如相似度计算

**数学本质**：嵌入是一个映射函数 $\phi: \mathcal{X} \rightarrow \mathbb{R}^d$，将高维稀疏空间映射到低维稠密空间，同时保持语义关系：
$$\text{sim}(x_i, x_j) \approx \text{sim}(\phi(x_i), \phi(x_j))$$

## Word2Vec：嵌入学习的开山之作 📚

### 核心思想：分布式假设

**基本原理**："你可以通过一个词的朋友来了解这个词"
- 在相似上下文中出现的词具有相似的含义
- 将这个思想用数学建模：$P(\text{context} | \text{word})$

### Skip-gram模型详解

**训练目标**：给定中心词，预测周围词
$$\mathcal{L} = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$

其中 $T$ 是语料总长度，$c$ 是窗口大小。

**概率建模**：
$$P(w_o | w_i) = \frac{\exp(\mathbf{v}_{w_o}^T \mathbf{v}_{w_i})}{\sum_{w=1}^{V} \exp(\mathbf{v}_w^T \mathbf{v}_{w_i})}$$

**完整实现：**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
import numpy as np

class SkipGram(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        # 中心词嵌入
        self.center_embeddings = nn.Embedding(vocab_size, embed_dim)
        # 上下文词嵌入
        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)
        
        # 初始化权重
        nn.init.uniform_(self.center_embeddings.weight, -0.5/embed_dim, 0.5/embed_dim)
        nn.init.uniform_(self.context_embeddings.weight, -0.5/embed_dim, 0.5/embed_dim)
    
    def forward(self, center_words, context_words, negative_words):
        # 获取嵌入向量
        center_embeds = self.center_embeddings(center_words)  # [batch, embed_dim]
        context_embeds = self.context_embeddings(context_words)  # [batch, embed_dim]
        neg_embeds = self.context_embeddings(negative_words)  # [batch, neg_num, embed_dim]
        
        # 正样本得分
        pos_score = torch.sum(center_embeds * context_embeds, dim=1)
        pos_loss = -F.logsigmoid(pos_score)
        
        # 负样本得分
        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze()
        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)
        
        return torch.mean(pos_loss + neg_loss)

class Word2VecTrainer:
    def __init__(self, sentences, min_count=5, window_size=5, embed_dim=100):
        self.sentences = sentences
        self.window_size = window_size
        self.embed_dim = embed_dim
        
        # 构建词汇表
        self.build_vocab(min_count)
        
        # 准备负采样表
        self.build_negative_sampling_table()
        
    def build_vocab(self, min_count):
        """构建词汇表"""
        word_count = Counter()
        for sentence in self.sentences:
            word_count.update(sentence.split())
        
        # 过滤低频词
        self.vocab = {word: idx for idx, (word, count) in 
                     enumerate(word_count.items()) if count >= min_count}
        self.idx2word = {idx: word for word, idx in self.vocab.items()}
        self.vocab_size = len(self.vocab)
        
    def build_negative_sampling_table(self, table_size=1e8):
        """构建负采样表"""
        # 计算词频的3/4次方
        word_counts = Counter()
        for sentence in self.sentences:
            for word in sentence.split():
                if word in self.vocab:
                    word_counts[word] += 1
        
        # 构建采样表
        power = 0.75
        norm = sum([count**power for count in word_counts.values()])
        table = []
        
        for word, count in word_counts.items():
            prob = (count**power) / norm
            table.extend([self.vocab[word]] * int(prob * table_size))
        
        self.negative_table = np.array(table)
        
    def get_negative_samples(self, target_word, num_negatives=5):
        """获取负样本"""
        negatives = []
        while len(negatives) < num_negatives:
            neg_word = np.random.choice(self.negative_table)
            if neg_word != target_word:
                negatives.append(neg_word)
        return negatives
    
    def generate_training_data(self):
        """生成训练数据"""
        training_data = []
        
        for sentence in self.sentences:
            words = [word for word in sentence.split() if word in self.vocab]
            
            for i, center_word in enumerate(words):
                center_idx = self.vocab[center_word]
                
                # 获取上下文窗口
                start = max(0, i - self.window_size)
                end = min(len(words), i + self.window_size + 1)
                
                for j in range(start, end):
                    if i != j:
                        context_word = words[j]
                        context_idx = self.vocab[context_word]
                        negatives = self.get_negative_samples(context_idx)
                        
                        training_data.append((center_idx, context_idx, negatives))
        
        return training_data

# 训练示例
def train_word2vec(sentences, epochs=10):
    trainer = Word2VecTrainer(sentences)
    model = SkipGram(trainer.vocab_size, trainer.embed_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    training_data = trainer.generate_training_data()
    
    for epoch in range(epochs):
        total_loss = 0
        
        # 打乱训练数据
        np.random.shuffle(training_data)
        
        for center, context, negatives in training_data:
            optimizer.zero_grad()
            
            center_tensor = torch.tensor([center])
            context_tensor = torch.tensor([context])
            neg_tensor = torch.tensor([negatives])
            
            loss = model(center_tensor, context_tensor, neg_tensor)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch {epoch+1}: Loss = {total_loss/len(training_data):.4f}')
    
    return model, trainer
```

### 负采样：计算效率的突破

**问题**：Softmax计算复杂度为 $O(V)$，词汇表很大时计算昂贵

**解决方案**：将多分类问题转化为二分类问题
- 正样本：真实的上下文词对
- 负样本：随机采样的"假"上下文词对

**负采样公式**：
$$\log \sigma(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-\mathbf{v}_{w_i}^T \mathbf{v}_{w_I})]$$

**采样分布**：$P_n(w) = \frac{U(w)^{3/4}}{Z}$，其中 $U(w)$ 是词频

**为什么用3/4次方？**
- 平衡高频词和低频词的采样概率
- 防止高频词过度采样，低频词采样不足

## Item2Vec：从文本到推荐的迁移 🛍️

### 核心思想

**类比转换：**
- **Word2Vec**：词 → 上下文词
- **Item2Vec**：物品 → 用户行为序列中的相邻物品

**分布式假设在推荐中的应用：**
"在相似用户行为序列中出现的物品具有相似的特性"

### 算法流程

**1. 构建物品序列**
```python
def build_item_sequences(user_behaviors):
    """从用户行为构建物品序列"""
    sequences = []
    for user_id, items in user_behaviors.items():
        # 按时间排序
        sorted_items = sorted(items, key=lambda x: x['timestamp'])
        item_sequence = [item['item_id'] for item in sorted_items]
        sequences.append(item_sequence)
    return sequences
```

**2. 完整Item2Vec实现**

```python
class Item2Vec:
    def __init__(self, sequences, embed_dim=128, window=5, min_count=5, 
                 negative=5, epochs=10, lr=0.001):
        self.sequences = sequences
        self.embed_dim = embed_dim
        self.window = window
        self.min_count = min_count
        self.negative = negative
        self.epochs = epochs
        self.lr = lr
        
        # 构建词汇表
        self.build_vocab()
        # 构建负采样表
        self.build_negative_table()
        # 初始化模型
        self.init_model()
    
    def build_vocab(self):
        """构建物品词汇表"""
        item_count = Counter()
        for seq in self.sequences:
            item_count.update(seq)
        
        # 过滤低频物品
        self.vocab = {}
        idx = 0
        for item, count in item_count.items():
            if count >= self.min_count:
                self.vocab[item] = idx
                idx += 1
        
        self.vocab_size = len(self.vocab)
        self.idx2item = {v: k for k, v in self.vocab.items()}
        print(f"词汇表大小: {self.vocab_size}")
    
    def build_negative_table(self):
        """构建负采样表"""
        item_count = Counter()
        for seq in self.sequences:
            for item in seq:
                if item in self.vocab:
                    item_count[item] += 1
        
        # 计算采样概率
        power = 0.75
        total = sum([count**power for count in item_count.values()])
        
        table = []
        for item, count in item_count.items():
            if item in self.vocab:
                prob = (count**power) / total
                table.extend([self.vocab[item]] * int(prob * 1e8))
        
        self.negative_table = np.array(table)
    
    def init_model(self):
        """初始化嵌入模型"""
        self.center_embeddings = np.random.uniform(
            -0.5/self.embed_dim, 0.5/self.embed_dim, 
            (self.vocab_size, self.embed_dim)
        ).astype(np.float32)
        
        self.context_embeddings = np.random.uniform(
            -0.5/self.embed_dim, 0.5/self.embed_dim,
            (self.vocab_size, self.embed_dim)
        ).astype(np.float32)
    
    def get_negative_samples(self, target_idx, num_samples):
        """获取负样本"""
        negatives = []
        while len(negatives) < num_samples:
            neg_idx = np.random.choice(self.negative_table)
            if neg_idx != target_idx:
                negatives.append(neg_idx)
        return negatives
    
    def train(self):
        """训练模型"""
        # 生成训练样本
        training_pairs = []
        
        for seq in self.sequences:
            # 过滤出词汇表中的物品
            seq_indices = [self.vocab[item] for item in seq if item in self.vocab]
            
            # 生成训练对
            for i, center_idx in enumerate(seq_indices):
                # 定义窗口范围
                start = max(0, i - self.window)
                end = min(len(seq_indices), i + self.window + 1)
                
                for j in range(start, end):
                    if i != j:
                        context_idx = seq_indices[j]
                        negatives = self.get_negative_samples(context_idx, self.negative)
                        training_pairs.append((center_idx, context_idx, negatives))
        
        print(f"生成训练样本: {len(training_pairs)}")
        
        # 训练循环
        for epoch in range(self.epochs):
            np.random.shuffle(training_pairs)
            total_loss = 0
            
            for center_idx, context_idx, negatives in training_pairs:
                # 计算梯度和更新权重
                loss = self.train_pair(center_idx, context_idx, negatives)
                total_loss += loss
            
            avg_loss = total_loss / len(training_pairs)
            print(f"Epoch {epoch+1}: Loss = {avg_loss:.6f}")
    
    def train_pair(self, center_idx, context_idx, negatives):
        """训练单个样本对"""
        center_vec = self.center_embeddings[center_idx]
        context_vec = self.context_embeddings[context_idx]
        
        # 正样本
        pos_score = np.dot(center_vec, context_vec)
        pos_prob = self.sigmoid(pos_score)
        
        # 计算正样本梯度
        pos_grad = (1 - pos_prob) * self.lr
        self.center_embeddings[center_idx] += pos_grad * context_vec
        self.context_embeddings[context_idx] += pos_grad * center_vec
        
        pos_loss = -np.log(pos_prob)
        
        # 负样本
        neg_loss = 0
        for neg_idx in negatives:
            neg_vec = self.context_embeddings[neg_idx]
            neg_score = np.dot(center_vec, neg_vec)
            neg_prob = self.sigmoid(neg_score)
            
            # 计算负样本梯度
            neg_grad = -neg_prob * self.lr
            self.center_embeddings[center_idx] += neg_grad * neg_vec
            self.context_embeddings[neg_idx] += neg_grad * center_vec
            
            neg_loss += -np.log(1 - neg_prob)
        
        return pos_loss + neg_loss
    
    def sigmoid(self, x):
        """Sigmoid函数"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def get_embedding(self, item):
        """获取物品嵌入"""
        if item in self.vocab:
            idx = self.vocab[item]
            return self.center_embeddings[idx]
        return None
    
    def similarity(self, item1, item2):
        """计算物品相似度"""
        emb1 = self.get_embedding(item1)
        emb2 = self.get_embedding(item2)
        
        if emb1 is not None and emb2 is not None:
            # 余弦相似度
            norm1 = np.linalg.norm(emb1)
            norm2 = np.linalg.norm(emb2)
            if norm1 > 0 and norm2 > 0:
                return np.dot(emb1, emb2) / (norm1 * norm2)
        return 0
    
    def most_similar(self, item, topk=10):
        """找到最相似的物品"""
        if item not in self.vocab:
            return []
        
        target_emb = self.get_embedding(item)
        similarities = []
        
        for other_item in self.vocab:
            if other_item != item:
                sim = self.similarity(item, other_item)
                similarities.append((other_item, sim))
        
        # 按相似度排序
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:topk]

# 使用示例
def train_item2vec_example():
    # 模拟用户行为数据
    user_behaviors = {
        'user1': [
            {'item_id': 'item_1', 'timestamp': 1},
            {'item_id': 'item_2', 'timestamp': 2},
            {'item_id': 'item_3', 'timestamp': 3}
        ],
        'user2': [
            {'item_id': 'item_2', 'timestamp': 1},
            {'item_id': 'item_4', 'timestamp': 2},
            {'item_id': 'item_5', 'timestamp': 3}
        ]
        # 更多用户数据...
    }
    
    # 构建序列
    sequences = build_item_sequences(user_behaviors)
    
    # 训练Item2Vec
    model = Item2Vec(sequences, embed_dim=64, epochs=20)
    model.train()
    
    # 测试相似度
    similar_items = model.most_similar('item_1', topk=5)
    print(f"与item_1最相似的物品: {similar_items}")
    
    return model
```

### 高效的Gensim实现

```python
from gensim.models import Word2Vec

def train_item2vec_gensim(item_sequences, embed_dim=128, window=5, min_count=5):
    """使用Gensim训练Item2Vec模型"""
    # 将item_id转换为字符串
    str_sequences = [[str(item) for item in seq] for seq in item_sequences]
    
    model = Word2Vec(
        sentences=str_sequences,
        vector_size=embed_dim,    # 嵌入维度
        window=window,            # 窗口大小
        min_count=min_count,      # 最小词频
        workers=4,                # 并行线程数
        sg=1,                     # 使用Skip-gram
        negative=5,               # 负采样数量
        epochs=10,
        alpha=0.025,              # 学习率
        min_alpha=0.0001          # 最小学习率
    )
    
    return model

def use_trained_model(model):
    """使用训练好的模型"""
    # 获取物品嵌入
    item_embedding = model.wv['item_1']
    
    # 计算相似度
    similar_items = model.wv.most_similar('item_1', topn=10)
    
    # 保存模型
    model.save('item2vec.model')
    
    # 加载模型
    loaded_model = Word2Vec.load('item2vec.model')
    
    return similar_items
```

## 嵌入空间的几何性质 📐

### 向量运算的语义解释

**向量加法的几何意义**：
对于类比关系 $a : b :: c : d$，有：
$\mathbf{v}_b - \mathbf{v}_a \approx \mathbf{v}_d - \mathbf{v}_c$

这等价于在嵌入空间中平移不变性。

**示例验证：**
```python
def test_analogy(model, word1, word2, word3):
    """测试类比关系 word1:word2 :: word3:?"""
    try:
        # 计算 word2 - word1 + word3
        result = model.wv.most_similar(
            positive=[word2, word3], 
            negative=[word1], 
            topn=1
        )
        return result[0][0]
    except:
        return None

# 测试示例
# king - man + woman ≈ queen
result = test_analogy(model, 'king', 'man', 'woman')
print(f"king - man + woman = {result}")
```

### 嵌入维度的选择

**经验规律**：
实践中最优维度通常为：
$d^* \approx \alpha \sqrt[4]{V}$，其中 $\alpha \in [10, 30]$，$V$ 是词汇表大小

**维度选择实验：**
```python
def dimension_experiment(sequences, dims=[32, 64, 128, 256]):
    """测试不同维度的效果"""
    results = {}
    
    for dim in dims:
        print(f"测试维度: {dim}")
        
        # 训练模型
        model = Item2Vec(sequences, embed_dim=dim, epochs=5)
        model.train()
        
        # 评估相似度质量（需要人工标注的相似度数据）
        # 这里简化为计算平均相似度
        avg_sim = evaluate_similarity_quality(model)
        results[dim] = avg_sim
        
        print(f"维度 {dim}: 平均相似度 = {avg_sim:.4f}")
    
    return results

def evaluate_similarity_quality(model):
    """评估相似度质量"""
    # 简化的评估方法：计算所有物品对的平均相似度
    similarities = []
    items = list(model.vocab.keys())[:100]  # 取前100个物品
    
    for i in range(len(items)):
        for j in range(i+1, len(items)):
            sim = model.similarity(items[i], items[j])
            similarities.append(sim)
    
    return np.mean(similarities)
```

## 实际应用中的技巧 🔧

### 冷启动问题处理

```python
class HybridItem2Vec:
    def __init__(self, item_features=None):
        self.item_features = item_features  # 物品内容特征
        self.behavior_model = None          # 行为序列模型
        self.content_model = None           # 内容特征模型
    
    def train_behavior_model(self, sequences):
        """训练基于行为的模型"""
        self.behavior_model = Item2Vec(sequences)
        self.behavior_model.train()
    
    def train_content_model(self, item_features):
        """训练基于内容的模型"""
        # 使用物品特征训练嵌入
        # 这里简化实现
        pass
    
    def get_embedding(self, item):
        """获取物品嵌入，处理冷启动"""
        # 首先尝试从行为模型获取
        if self.behavior_model and item in self.behavior_model.vocab:
            return self.behavior_model.get_embedding(item)
        
        # 如果是新物品，使用内容特征
        elif self.content_model and item in self.item_features:
            return self.content_model.get_embedding(item)
        
        # 都没有则返回随机嵌入或平均嵌入
        else:
            return np.random.normal(0, 0.1, self.behavior_model.embed_dim)
```

### 增量训练

```python
class IncrementalItem2Vec:
    def __init__(self, model_path=None):
        if model_path:
            self.model = Word2Vec.load(model_path)
        else:
            self.model = None
    
    def update_with_new_sequences(self, new_sequences):
        """使用新序列增量更新模型"""
        if self.model is None:
            # 首次训练
            self.model = Word2Vec(
                new_sequences, vector_size=128, window=5, 
                min_count=5, workers=4, sg=1
            )
        else:
            # 增量训练
            self.model.build_vocab(new_sequences, update=True)
            self.model.train(new_sequences, total_examples=len(new_sequences), epochs=5)
    
    def save_model(self, path):
        """保存模型"""
        self.model.save(path)
```

### 多模态嵌入

```python
class MultiModalEmbedding:
    def __init__(self, embed_dim=128):
        self.embed_dim = embed_dim
        self.behavior_embeds = {}    # 行为嵌入
        self.content_embeds = {}     # 内容嵌入
        self.fusion_weights = None   # 融合权重
    
    def train_behavior_embedding(self, sequences):
        """训练行为嵌入"""
        model = Item2Vec(sequences, embed_dim=self.embed_dim)
        model.train()
        
        for item in model.vocab:
            self.behavior_embeds[item] = model.get_embedding(item)
    
    def train_content_embedding(self, item_features):
        """训练内容嵌入"""
        # 使用物品特征训练内容嵌入
        # 这里可以使用各种方法：TF-IDF + 降维，预训练模型等
        pass
    
    def fuse_embeddings(self, item, alpha=0.7):
        """融合多种嵌入"""
        behavior_emb = self.behavior_embeds.get(item)
        content_emb = self.content_embeds.get(item)
        
        if behavior_emb is not None and content_emb is not None:
            # 加权平均
            return alpha * behavior_emb + (1 - alpha) * content_emb
        elif behavior_emb is not None:
            return behavior_emb
        elif content_emb is not None:
            return content_emb
        else:
            return np.zeros(self.embed_dim)
```

## 评估与优化 📊

### 嵌入质量评估

```python
def evaluate_embedding_quality(model, test_similarities=None):
    """评估嵌入质量"""
    metrics = {}
    
    # 1. 内在评估：词类比任务
    if hasattr(model, 'evaluate_word_analogies'):
        analogy_score = model.evaluate_word_analogies('analogies.txt')
        metrics['analogy_accuracy'] = analogy_score
    
    # 2. 外在评估：推荐任务性能
    if test_similarities:
        pred_sims = []
        true_sims = []
        
        for (item1, item2), true_sim in test_similarities.items():
            pred_sim = model.similarity(item1, item2)
            pred_sims.append(pred_sim)
            true_sims.append(true_sim)
        
        from scipy.stats import spearmanr
        correlation, p_value = spearmanr(pred_sims, true_sims)
        metrics['similarity_correlation'] = correlation
    
    # 3. 嵌入空间分析
    embeddings = np.array([model.get_embedding(item) for item in model.vocab if model.get_embedding(item) is not None])
    
    # 计算有效维度
    pca = PCA()
    pca.fit(embeddings)
    explained_var_ratio = pca.explained_variance_ratio_
    
    # 90%方差对应的维度
    cumsum_ratio = np.cumsum(explained_var_ratio)
    effective_dim = np.argmax(cumsum_ratio >= 0.9) + 1
    metrics['effective_dimension'] = effective_dim
    
    return metrics

# 可视化嵌入
def visualize_embeddings(model, items=None, method='tsne'):
    """可视化嵌入向量"""
    if items is None:
        items = list(model.vocab.keys())[:100]  # 取前100个
    
    embeddings = np.array([model.get_embedding(item) for item in items])
    
    if method == 'tsne':
        from sklearn.manifold import TSNE
        embeddings_2d = TSNE(n_components=2, random_state=42).fit_transform(embeddings)
    elif method == 'pca':
        from sklearn.decomposition import PCA
        embeddings_2d = PCA(n_components=2).fit_transform(embeddings)
    
    # 绘图
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 8))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)
    
    # 标注部分物品
    for i in range(min(20, len(items))):
        plt.annotate(items[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]))
    
    plt.title(f'Item Embeddings Visualization ({method.upper()})')
    plt.show()
```

## 实践总结 🎯

### 最佳实践

1. **数据预处理**：
   - 过滤低频物品（min_count参数）
   - 序列长度截断（避免过长序列）
   - 时间窗口内的行为聚合

2. **参数调优**：
   - 嵌入维度：$\sqrt[4]{\text{vocab_size}} \times [10, 30]$
   - 窗口大小：3-10，根据业务场景调整
   - 负采样数量：5-20

3. **工程优化**：
   - 使用成熟库（Gensim, FastText）
   - 增量训练支持
   - 多模态信息融合

### 常见问题解决

1. **训练不收敛**：
   - 降低学习率
   - 增加训练轮数
   - 检查数据质量

2. **相似度效果差**：
   - 增加训练数据
   - 调整窗口大小
   - 尝试不同的负采样策略

3. **冷启动问题**：
   - 结合内容特征
   - 使用物品分类信息
   - 集成多种嵌入方法

> 💡 **关键洞察**
> 
> Embedding的成功在于：
> - 分布式假设的合理性：相似上下文 → 相似语义
> - 负采样的计算效率：从O(V)降低到O(k)
> - 向量空间的几何性质：支持语义运算
> - 迁移学习的可能性：Word2Vec → Item2Vec

通过本章的学习，您应该掌握了Embedding技术的核心原理和实现方法。这为理解更复杂的深度推荐模型奠定了坚实基础。
