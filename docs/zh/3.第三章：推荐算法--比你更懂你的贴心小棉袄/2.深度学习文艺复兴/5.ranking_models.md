---
title: æ·±åº¦æ’åºæ¨¡å‹ï¼šç²¾è°ƒæµé‡çš„"ç‚¼é‡‘æœ¯"
createTime: 2025/06/13 14:00
---

å¦‚æœè¯´å¬å›æ˜¯ä»äº¿ä¸‡ç‰©æ–™ä¸­"å¤§æµ·æé’ˆ"ï¼Œé‚£ä¹ˆ**æ’åºï¼ˆRankingï¼‰** å°±æ˜¯åœ¨è¿™å‡ ç™¾æ ¹"é’ˆ"é‡Œæ‰¾å‡ºæœ€äº®çš„é‚£ä¸€æ ¹ã€‚å®ƒæ˜¯æ¨èæµç¨‹çš„"æœ€åä¸€å…¬é‡Œ"ï¼Œè´Ÿè´£å¯¹å¬å›çš„å€™é€‰é›†è¿›è¡Œç²¾å‡†æ‰“åˆ†ï¼Œé¢„æµ‹ç”¨æˆ·å¯¹æ¯ä¸ªç‰©å“çš„åå¥½ç¨‹åº¦ï¼ˆå¦‚ç‚¹å‡»ç‡ pCTRã€è½¬åŒ–ç‡ pCVRï¼‰ï¼Œç„¶åæŒ‰åˆ†å€¼é«˜ä½å‘ˆç°ç»™ç”¨æˆ·ã€‚

æ’åºæ¨¡å‹çš„å¥½åï¼Œç›´æ¥å†³å®šäº†ç”¨æˆ·çš„æœ€ç»ˆä½“éªŒå’Œå¹³å°çš„å•†ä¸šç›®æ ‡ã€‚è¿™æ˜¯ä¸€ä¸ªäºç»†å¾®å¤„è§çœŸç« çš„"ç‚¼é‡‘"è¿‡ç¨‹ï¼Œæ¨¡å‹çš„æ¯ä¸€æ¬¡å¾®å°è¿­ä»£ï¼Œéƒ½å¯èƒ½å¸¦æ¥å·¨å¤§çš„æ”¶ç›Šã€‚

## ğŸ¯ æ’åºæ¨¡å‹çš„ä¸€èˆ¬æ•°å­¦æ¡†æ¶

ä»æ•°å­¦ä¸Šçœ‹ï¼Œå¤§å¤šæ•° CTR/CVR æ’åºæ¨¡å‹éƒ½å¯ä»¥æŠ½è±¡ä¸ºä¸€æ¡æµæ°´çº¿ï¼š

> **ç‰¹å¾ $\to$ Embedding/äº¤å‰ $\to$ DNN è¡¨ç¤º $\to$ logit $z$ $\to$ æ¦‚ç‡ $p$ $\to$ æŸå¤± $\mathcal{L}$**

### ç‰¹å¾ä¸ Embedding è¡¨ç¤º

è®¾ä¸€æ¡æ ·æœ¬åŒ…å«ï¼š
- ç¨ å¯†ç‰¹å¾ï¼ˆå¦‚æ•°å€¼ç±»ç‰¹å¾ï¼‰ $\mathbf{x}_{\text{dense}}\in\mathbb{R}^{d_{\text{dense}}}$ï¼›
- $m$ ä¸ªç¦»æ•£ç‰¹å¾åŸŸï¼ˆå¦‚ç”¨æˆ· IDã€å•†å“ç±»ç›®ã€åŸå¸‚ç­‰ï¼‰ $f_1,\dots,f_m$ã€‚

æ¯ä¸ªç¦»æ•£ç‰¹å¾åŸŸé€šè¿‡ Embedding æ˜ å°„åˆ°ä½ç»´å‘é‡ç©ºé—´ï¼š

$$
\mathbf{v}_k = \text{Emb}_k(f_k) \in \mathbb{R}^d, \quad k=1,\dots,m.
$$

å°†æ‰€æœ‰åŸŸçš„å‘é‡ä¸ç¨ å¯†ç‰¹å¾æ‹¼æ¥ï¼Œå¾—åˆ°æ’åºæ¨¡å‹çš„åŸºç¡€è¾“å…¥ï¼š

$$
\mathbf{e} = [\mathbf{v}_1;\dots;\mathbf{v}_m;\mathbf{x}_{\text{dense}}] \in \mathbb{R}^{d_e}.
$$

è¿™é‡Œ $d$ æ˜¯ Embedding ç»´åº¦ï¼Œ$d_e$ æ˜¯æ‹¼æ¥åçš„æ€»ç»´åº¦ã€‚

### DNN ä¸ç‰¹å¾äº¤å‰

åœ¨æœ€ç®€å•çš„â€œçº¯ DNN æ’åºæ¨¡å‹â€ä¸­ï¼Œæˆ‘ä»¬å°† $\mathbf{e}$ é€å…¥å¤šå±‚å‰é¦ˆç½‘ç»œï¼š

$$
\begin{aligned}
\mathbf{h}^{(0)} &= \mathbf{e},\\
\mathbf{h}^{(\ell)} &= \sigma\big(W^{(\ell)}\mathbf{h}^{(\ell-1)} + \mathbf{b}^{(\ell)}\big), \quad \ell=1,\dots,L,\\
\end{aligned}
$$

å…¶ä¸­ $W^{(\ell)}\in\mathbb{R}^{d_{\ell}\times d_{\ell-1}}$ï¼Œ$\sigma$ ä¸ºéçº¿æ€§æ¿€æ´»ï¼ˆReLUã€PReLU ç­‰ï¼‰ã€‚

Wide&Deepã€DeepFMã€DCN ç­‰æ¨¡å‹çš„åŒºåˆ«ï¼Œä¸»è¦ä½“ç°åœ¨ **å¦‚ä½•åœ¨ $\mathbf{e}$ çš„åŸºç¡€ä¸Šæ˜¾å¼/éšå¼åœ°æ„é€ ç‰¹å¾äº¤å‰**ï¼š
- Wide&Deepï¼šåœ¨ $\mathbf{e}$ ä¹‹å¤–ï¼Œé¢å¤–å¼•å…¥çº¿æ€§çš„ Wide äº¤å‰ç‰¹å¾ï¼›
- DeepFMï¼šç”¨ FM æ˜¾å¼å»ºæ¨¡äºŒé˜¶äº¤å‰ï¼ŒåŒæ—¶ç”¨ DNN å­¦ä¹ é«˜é˜¶äº¤å‰ï¼›
- DCNï¼šç”¨ Cross Network æ˜¾å¼æ„é€ å¤šé˜¶äº¤å‰å±‚ï¼›
- DIN/DIENï¼šé€šè¿‡æ³¨æ„åŠ›å’Œåºåˆ—ç½‘ç»œï¼Œä»ç”¨æˆ·å†å²åºåˆ—ä¸­æå–ä¸å½“å‰å€™é€‰å¼ºç›¸å…³çš„â€œå…´è¶£ç‰¹å¾â€ã€‚

### ä» logit åˆ° CTR æ¦‚ç‡

æ— è®ºç‰¹å¾äº¤å‰å¦‚ä½•è®¾è®¡ï¼Œæœ€ç»ˆéƒ½ä¼šå¾—åˆ°æŸä¸€å±‚çš„è¾“å‡º $\mathbf{h}^{(L)}$ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°æ ‡é‡ logitï¼š

$$
 z = w^\top \mathbf{h}^{(L)} + b, \quad w\in\mathbb{R}^{d_L}, b\in\mathbb{R}.
$$

å¹¶é€šè¿‡ **sigmoid** å¾—åˆ°ç‚¹å‡»æ¦‚ç‡é¢„æµ‹ï¼š

$$
 p = p(y=1\mid x) = \sigma(z) = \frac{1}{1+e^{-z}}.
$$

è¿™é‡Œ $y\in\{0,1\}$ è¡¨ç¤ºæ˜¯å¦ç‚¹å‡»ï¼ˆæˆ–è½¬åŒ–ï¼‰ï¼Œ$p$ å°±æ˜¯æ¨¡å‹å¯¹ CTR çš„ä¼°è®¡å€¼ã€‚

### æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–ç›®æ ‡

åœ¨æ’åºé˜¶æ®µï¼Œå¸¸è§çš„æŸå¤±åŒ…æ‹¬ï¼š

1. **äºŒåˆ†ç±»äº¤å‰ç†µï¼ˆBinary Cross Entropyï¼‰**ï¼š

$$
\mathcal{L}_{\text{BCE}} = -\frac{1}{N}\sum_{n=1}^N\Big[y_n\log p_n + (1-y_n)\log(1-p_n)\Big].
$$

å¯ä»¥æ¨å¾—å¯¹ logit çš„æ¢¯åº¦ï¼š

$$
\frac{\partial \mathcal{L}_{\text{BCE}}}{\partial z_n} = p_n - y_n,
$$

è¿™æ„å‘³ç€æ’åºæ¨¡å‹åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„æ›´æ–°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯â€œ**é¢„æµ‹æ¦‚ç‡å‡å»çœŸå®æ ‡ç­¾**â€ï¼Œè¿™ä¸€ç®€å•å½¢å¼æå¤§åœ°æ–¹ä¾¿äº†åå‘ä¼ æ’­çš„å®ç°ä¸åˆ†æã€‚

2. **åŠ æƒäº¤å‰ç†µï¼ˆWeighted BCEï¼‰**ï¼šç”¨äºå¤„ç†æ­£è´Ÿæ ·æœ¬æåº¦ä¸å¹³è¡¡æˆ–è€…ä¸šåŠ¡é‡è§†ç¨‹åº¦ä¸åŒçš„æƒ…å½¢ï¼š

$$
\mathcal{L}_{\text{WBCE}} = -\frac{1}{N}\sum_{n=1}^N\Big[w_1 y_n\log p_n + w_0(1-y_n)\log(1-p_n)\Big],
$$

å…¶ä¸­ $w_1,w_0$ ä¸ºæ­£è´Ÿæ ·æœ¬æƒé‡ï¼Œç”¨äºåœ¨æ¢¯åº¦å±‚é¢æ”¾å¤§â€œæ›´å…³å¿ƒâ€çš„æ ·æœ¬ã€‚

3. **Focal Loss**ï¼šåœ¨æåº¦ä¸å¹³è¡¡åœºæ™¯ï¼ˆå¦‚æ›å…‰å¾ˆå¤šä½†ç‚¹å‡»å¾ˆå°‘ï¼‰ä¸­å¸¸ç”¨ï¼Œç”¨äºâ€œèšç„¦éš¾æ ·æœ¬â€ï¼š

$$
\begin{aligned}
\mathcal{L}_{\text{Focal}}
&= -\frac{1}{N}\sum_{n=1}^N \Big[\alpha(1-p_n)^\gamma y_n\log p_n\\
&\quad\quad + (1-\alpha)p_n^\gamma (1-y_n)\log(1-p_n)\Big],
\end{aligned}
$$

å…¶ä¸­ $\gamma>0$ æ§åˆ¶â€œéš¾æ ·æœ¬åŠ æƒâ€çš„å¼ºåº¦ï¼Œ$\alpha\in(0,1)$ æ§åˆ¶æ­£è´Ÿæ ·æœ¬æ•´ä½“æƒé‡ã€‚

> ç›´è§‚ç†è§£ï¼šå½“ä¸€ä¸ªæ ·æœ¬å·²ç»è¢«é¢„æµ‹å¾—å¾ˆå¥½ï¼ˆå¦‚ $p_n$ æ¥è¿‘æ ‡ç­¾ï¼‰ï¼Œ$(1-p_n)^\gamma$ æˆ– $p_n^\gamma$ ä¼šè®©å®ƒçš„æŸå¤±å¤§å¹…ç¼©å°ï¼Œä»è€ŒæŠŠâ€œæ³¨æ„åŠ›â€æ›´å¤šåœ°ç•™ç»™é‚£äº›é¢„æµ‹ä¸å¥½çš„æ ·æœ¬ã€‚

### ä¸å¬å›é˜¶æ®µçš„æ•°å­¦å·®å¼‚

ä¸ä¸Šä¸€èŠ‚çš„æ·±åº¦å¬å›ä¸åŒï¼š
- **å¬å›é˜¶æ®µ**ï¼šå…¸å‹åšæ³•æ˜¯åœ¨å€™é€‰é›†åˆï¼ˆæ­£æ ·æœ¬ + è´Ÿæ ·æœ¬ï¼‰ä¸Šåš **softmax**ï¼Œç›®æ ‡æ˜¯è®©â€œæ­£ç¡®çš„é‚£ä¸ªç‰©å“â€åœ¨è¯¥é›†åˆä¸­çš„æ¦‚ç‡æœ€å¤§ï¼›
- **æ’åºé˜¶æ®µ**ï¼šé¢å¯¹çš„æ˜¯å·²ç»è¾ƒå°çš„ä¸€ç»„å€™é€‰ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬åš **äºŒåˆ†ç±»**ï¼ˆæ˜¯å¦ç‚¹å‡»/è½¬åŒ–ï¼‰ï¼Œå…¸å‹æŸå¤±æ˜¯äº¤å‰ç†µæˆ–å…¶å˜ä½“ï¼›
- åœ¨å®ç°ä¸Šï¼Œå¬å›æ›´å…³å¿ƒ**å‘é‡æ£€ç´¢æ•ˆç‡**ï¼Œæ’åºæ›´å…³å¿ƒ**é¢„æµ‹æ ¡å‡†ã€ç‰¹å¾è¡¨è¾¾åŠ›ä¸å¤šç›®æ ‡æƒè¡¡**ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ²¿ç€è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œä¾æ¬¡èµ°è¿‘ Wide&Deepã€DeepFMã€DCNã€MMoE ç­‰ç»å…¸æ’åºæ¨¡å‹ï¼›è€ŒåŸºäºæ³¨æ„åŠ›å’Œåºåˆ—å»ºæ¨¡çš„ DIN/DIEN/BSTï¼Œå°†åœ¨ä¸‹ä¸€èŠ‚çš„æ³¨æ„åŠ›æ¨¡å‹ç« èŠ‚ä¸­å•ç‹¬å±•å¼€ã€‚

## ğŸ›ï¸ æ¨¡å‹æ¼”è¿›ä¹‹è·¯ï¼šä»åˆ†ç«‹åˆ°èåˆï¼Œä»äººå·¥åˆ°è‡ªåŠ¨

æ·±åº¦æ’åºæ¨¡å‹çš„æ¼”è¿›ï¼Œæ˜¯ä¸€éƒ¨è¿½æ±‚"æ›´é«˜ã€æ›´å¿«ã€æ›´å¼º"çš„ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ç»“æ„åˆ›æ–°å²ã€‚

### å¥ åŸºæ—¶ä»£ï¼šWide & Deep â€”â€” "è®°å¿†"ä¸"æ³›åŒ–"çš„é¦–æ¬¡è”å§»

Googleåœ¨2016å¹´æå‡ºçš„ `Wide & Deep` æ¨¡å‹ï¼Œæ˜¯æ·±åº¦å­¦ä¹ åœ¨æ¨èé¢†åŸŸé‡Œç¨‹ç¢‘å¼çš„ä½œå“ï¼Œå®ƒå¥ å®šäº†ç°ä»£æ’åºæ¨¡å‹çš„åŸºæœ¬èŒƒå¼ã€‚

-   **æ ¸å¿ƒæ€æƒ³**ï¼šä»»ä½•æ¨èåœºæ™¯éƒ½ç¦»ä¸å¼€"è®°å¿†"ï¼ˆMemorizationï¼‰å’Œ"æ³›åŒ–"ï¼ˆGeneralizationï¼‰ã€‚
    -   **Wideéƒ¨åˆ†ï¼ˆè®°å¿†ï¼‰**ï¼šä¸€ä¸ªå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œè´Ÿè´£è®°å¿†é‚£äº›é¢‘ç¹å…±ç°çš„ã€é‡è¦çš„äº¤å‰ç‰¹å¾ï¼Œæ¯”å¦‚"ç”¨æˆ·çˆ±å–æ‹¿é“ & åŸå¸‚æ˜¯ä¸Šæµ·"ã€‚å®ƒçš„ä¼˜ç‚¹æ˜¯ç®€å•ã€å¯è§£é‡Šæ€§å¼ºã€‚
    -   **Deepéƒ¨åˆ†ï¼ˆæ³›åŒ–ï¼‰**ï¼šä¸€ä¸ªæ ‡å‡†çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œè´Ÿè´£å°†ç¨€ç–ç‰¹å¾åµŒå…¥åˆ°ä½ç»´ç¨ å¯†ç©ºé—´ï¼Œå­¦ä¹ é‚£äº›ä»æœªæˆ–å¾ˆå°‘å‡ºç°çš„ç‰¹å¾ç»„åˆï¼Œæ¢ç´¢æœªçŸ¥çš„å¯èƒ½æ€§ã€‚
-   **ç²¾é«“**ï¼šå°†ä¸¤è€…è”åˆè®­ç»ƒï¼Œè®©æ¨¡å‹åŒæ—¶å…·å¤‡è®°ä½"å†å²ç»éªŒ"å’Œæ¢ç´¢"æ–°é²œäº‹ç‰©"çš„èƒ½åŠ›ã€‚

#### ğŸ§® æ•°å­¦å»ºæ¨¡ï¼šWide è®°å¿† + Deep æ³›åŒ–

è®¾é€šè¿‡ä¸Šä¸€å°èŠ‚å¾—åˆ°çš„æ‹¼æ¥å‘é‡ä¸º $\mathbf{e}$ï¼ŒåŒæ—¶äººä¸ºæ„é€ äº†ä¸€ç»„ Wide ä¾§çš„äº¤å‰ç‰¹å¾å‘é‡ $\boldsymbol{\phi}(x)\in\mathbb{R}^{d_w}$ï¼ˆå¦‚ã€Œç”¨æˆ·çˆ±å–æ‹¿é“ & åŸå¸‚æ˜¯ä¸Šæµ·ã€ï¼‰ã€‚

- **Wide éƒ¨åˆ†** æ˜¯ä¸€ä¸ªå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼š
  $$
  z_{\text{wide}} = \mathbf{w}_w^\top \boldsymbol{\phi}(x) + b_w,
  $$
  å…¶ä¸­ $\mathbf{w}_w\in\mathbb{R}^{d_w}$ã€$b_w\in\mathbb{R}$ ä¸ºå¾…å­¦ä¹ å‚æ•°ã€‚

- **Deep éƒ¨åˆ†** å¯¹ $\mathbf{e}$ åšå¤šå±‚éçº¿æ€§å˜æ¢ï¼š
  $$
  \begin{aligned}
  \mathbf{h}^{(0)} &= \mathbf{e},\\
  \mathbf{h}^{(\ell)} &= \sigma\big(W^{(\ell)}\mathbf{h}^{(\ell-1)} + \mathbf{b}^{(\ell)}\big), \; \ell=1,\dots,L,\\
  z_{\text{deep}} &= \mathbf{w}_d^\top \mathbf{h}^{(L)} + b_d.
  \end{aligned}
  $$

- **æ€» logit** ä¸ºä¸¤éƒ¨åˆ†ä¹‹å’Œï¼š
  $$
  z = z_{\text{wide}} + z_{\text{deep}}, \quad p = \sigma(z).
  $$

åœ¨äº¤å‰ç†µæŸå¤±ä¸‹ï¼ŒWide éƒ¨åˆ†å‚æ•°çš„æ¢¯åº¦å½¢å¦‚ï¼š
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}_w} = (p-y)\, \boldsymbol{\phi}(x),
$$
è¿™è¯´æ˜ï¼š
- å‡ºç°é¢‘ç‡é«˜ã€ä¸ç›®æ ‡å¼ºç›¸å…³çš„äº¤å‰ç‰¹å¾ï¼Œå…¶æ¢¯åº¦ç´¯è®¡æ•ˆåº”æ˜¾è‘—ï¼Œæ›´å®¹æ˜“è¢« **â€œè®°ä½â€**ï¼›
- Deep éƒ¨åˆ†åˆ™é€šè¿‡ DNN åœ¨å‘é‡ç©ºé—´ä¸­æ•è·ä»æœªè§è¿‡çš„ç»„åˆï¼Œå®ç° **â€œæ³›åŒ–â€**ã€‚

ä»è€Œåœ¨åŒä¸€å¥—å‚æ•°ä¸­å…¼é¡¾äº†ã€Œè§å¤šè¯†å¹¿ã€ä¸ã€Œè§¦ç±»æ—é€šã€ã€‚

::: details Wide&Deepæ¨¡å‹ç¤ºæ„å›¾
```mermaid
graph TD
    subgraph "Wide Part"
        direction LR
        W_FE["Wideä¾§ç‰¹å¾<br/>å¦‚äº¤å‰ç‰¹å¾"] --> W_LM["å¹¿ä¹‰çº¿æ€§æ¨¡å‹"]
    end

    subgraph "Deep Part"
        direction LR
        D_FE["Deepä¾§ç‰¹å¾<br/>å¦‚IDç±», å±æ€§ç±»"] --> D_Emb["Embeddingå±‚"] --> D_DNN["DNN"]
    end

    W_LM --> Sum
    D_DNN --> Sum

    Sum["\+"] --> Output["Sigmoidè¾“å‡º"]

```
:::

![Wide&Deepæ¨¡å‹ç»“æ„å›¾](/img/ch03/model_WideDeep.png)

::: details ğŸ’» Wide&Deep æ¨¡å‹å®ç° 
```python
import torch
import torch.nn as nn
from torch_rechub.models.layers import MLP, EmbeddingLayer

class WideDeep(nn.Module):
    """
    Wide & Deep Model
    
    Args:
        wide_features (list): Wideä¾§çš„ç‰¹å¾åˆ—è¡¨.
        deep_features (list): Deepä¾§çš„ç‰¹å¾åˆ—è¡¨.
        mlp_params (dict): Deepä¾§MLPçš„å‚æ•°.
    """
    def __init__(self, wide_features, deep_features, mlp_params):
        super().__init__()
        
        # Wideéƒ¨åˆ†
        self.wide_feature_layer = EmbeddingLayer(wide_features)
        
        # Deepéƒ¨åˆ†
        self.deep_feature_layer = EmbeddingLayer(deep_features)
        self.deep_mlp = MLP(self.deep_feature_layer.input_dim, **mlp_params)
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        self.output_layer = nn.Linear(mlp_params["output_dim"] + self.wide_feature_layer.input_dim, 1)

    def forward(self, x):
        """
        Args:
            x (dict): è¾“å…¥ç‰¹å¾, a dictionary with keys being feature names.
        """
        # Wideä¾§é€»è¾‘
        wide_input = self.wide_feature_layer(x) # (batch_size, wide_embed_dim)
        
        # Deepä¾§é€»è¾‘
        deep_input = self.deep_feature_layer(x) # (batch_size, deep_embed_dim)
        deep_output = self.deep_mlp(deep_input) # (batch_size, mlp_output_dim)
        
        # æ‹¼æ¥Wideå’ŒDeepçš„è¾“å‡º
        concat_out = torch.cat([wide_input, deep_output], dim=1)
        
        # æœ€ç»ˆé¢„æµ‹
        y_pred = self.output_layer(concat_out)
        return torch.sigmoid(y_pred)
```
:::

### äº¤å‰æ—¶ä»£ï¼šå‘Šåˆ«"äººå·¥ç‚¼ä¸¹"ï¼Œæ‹¥æŠ±"è‡ªåŠ¨é©¾é©¶"

`Wide & Deep` çš„ä¸€ä¸ªç—›ç‚¹æ˜¯ï¼ŒWideéƒ¨åˆ†çš„äº¤å‰ç‰¹å¾ä»ç„¶éœ€è¦å¤§é‡çš„äººå·¥è®¾è®¡å’Œç­›é€‰ï¼Œè¿™æ˜¯ä¸€é¡¹ç¹çä¸”ç»éªŒä¾èµ–çš„"ç‚¼ä¸¹"å·¥ä½œã€‚äºæ˜¯ï¼Œä¸€ç³»åˆ—èƒ½**è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤å‰**çš„æ¨¡å‹åº”è¿è€Œç”Ÿã€‚

#### DeepFMï¼šFMä¸DNNçš„ç è”ç’§åˆ

`DeepFM` æ˜¯è¿™ä¸ªæ—¶ä»£æœ€è€€çœ¼çš„æ˜æ˜Ÿï¼Œè‡³ä»Šä»æ˜¯è®¸å¤šå…¬å¸çº¿ä¸Šæ¨¡å‹çš„ä¸»åŠ›ã€‚å®ƒå·§å¦™åœ°å°†å› å­åˆ†è§£æœºï¼ˆFMï¼‰å’ŒDNNç»“åˆåœ¨äº†ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚

-   **æ ¸å¿ƒæ€æƒ³**ï¼š
    -   **FMéƒ¨åˆ†**ï¼šä¸Wideéƒ¨åˆ†ç±»ä¼¼ï¼Œä½†å®ƒèƒ½è‡ªåŠ¨å­¦ä¹ æ‰€æœ‰ç‰¹å¾ä¹‹é—´çš„äºŒé˜¶äº¤å‰ï¼Œæ— éœ€äººå·¥æŒ‡å®šã€‚
    -   **Deepéƒ¨åˆ†**ï¼šä¸`Wide & Deep`ä¸­çš„Deepéƒ¨åˆ†ä¸€è‡´ï¼Œå­¦ä¹ é«˜é˜¶äº¤å‰ç‰¹å¾ã€‚
-   **ç²¾é«“**ï¼šFMéƒ¨åˆ†å’ŒDeepéƒ¨åˆ†çš„è¾“å…¥Embeddingæ˜¯**å…±äº«**çš„ï¼è¿™å¸¦æ¥äº†ä¸¤å¤§å¥½å¤„ï¼š
    1.  æ¨¡å‹æ›´å°ï¼Œè®­ç»ƒæ›´å¿«ã€‚
    2.  Embeddingå±‚èƒ½è¢«ä½é˜¶å’Œé«˜é˜¶äº¤å‰ä¿¡å·åŒæ—¶è®­ç»ƒï¼Œå­¦ä¹ æ›´å……åˆ†ã€‚

#### ğŸ§® æ•°å­¦å»ºæ¨¡ï¼šFM + DNN çš„å…±äº« Embedding

è®¾å…±æœ‰ $m$ ä¸ªç‰¹å¾åŸŸï¼Œæ¯ä¸ªåŸŸçš„ one-hot/multi-hot è¾“å…¥è®°ä¸º $x_i$ï¼Œå…±äº«çš„ Embedding å‘é‡ä¸º $\mathbf{v}_i\in\mathbb{R}^d$ã€‚åˆ™ FM éƒ¨åˆ†çš„è¾“å‡ºå¯ä»¥å†™æˆï¼š

- **ä¸€é˜¶é¡¹**ï¼ˆç±»ä¼¼ Wide éƒ¨åˆ†ï¼‰ï¼š
  $$
  y_{\text{lin}} = w_0 + \sum_{i=1}^m w_i x_i;
  $$
- **äºŒé˜¶äº¤å‰é¡¹**ï¼š
  $$
  y_{\text{FM}}^{(2)} = \sum_{1\le i<j\le m} \langle \mathbf{v}_i, \mathbf{v}_j\rangle x_i x_j,
  $$
  å…¶ä¸­ $\langle \mathbf{v}_i, \mathbf{v}_j\rangle = \mathbf{v}_i^\top\mathbf{v}_j$ ä¸ºå†…ç§¯ã€‚

åœ¨å®ç°æ—¶å¸¸ç”¨ç­‰ä»·çš„å‘é‡åŒ–å½¢å¼ï¼š

$$
\begin{aligned}
\sum_{1\le i<j\le m} \langle \mathbf{v}_i, \mathbf{v}_j\rangle x_i x_j
&= \frac{1}{2}\Big[\big(\sum_{i=1}^m x_i\mathbf{v}_i\big)^2 - \sum_{i=1}^m x_i^2\mathbf{v}_i^{\odot 2}\Big],
\end{aligned}
$$

å…¶ä¸­ $\mathbf{v}_i^{\odot 2}$ è¡¨ç¤ºé€å…ƒç´ å¹³æ–¹ï¼Œæ‹¬å·å¹³æ–¹ä¸ºé€å…ƒç´ å¹³æ–¹å†æ±‚å’Œã€‚äºæ˜¯ FM çš„æ€»è¾“å‡ºä¸ºï¼š

$$
 y_{\text{FM}} = y_{\text{lin}} + y_{\text{FM}}^{(2)}.
$$

å¦ä¸€æ–¹é¢ï¼Œ**Deep éƒ¨åˆ†** ä½¿ç”¨åŒä¸€æ‰¹ Embeddingï¼Œå°†æ‰€æœ‰ $\mathbf{v}_i$ å±•å¹³æ‹¼æ¥ä½œä¸º DNN è¾“å…¥ï¼š

$$
\mathbf{e}_{\text{deep}} = [\mathbf{v}_1;\dots;\mathbf{v}_m;\mathbf{x}_{\text{dense}}],
$$

å¹¶ç»è¿‡å¤šå±‚ MLP å¾—åˆ°è¡¨ç¤º $\mathbf{h}^{(L)}$ï¼Œå†çº¿æ€§æ˜ å°„ä¸º
$$
 z_{\text{deep}} = \mathbf{w}_d^\top \mathbf{h}^{(L)} + b_d.
$$

æœ€ç»ˆ DeepFM çš„ logit ä¸ºï¼š
$$
 z = y_{\text{FM}} + z_{\text{deep}}, \quad p = \sigma(z).
$$

ä»æ¢¯åº¦è§’åº¦çœ‹ï¼ŒåŒä¸€ä¸ª Embedding å‘é‡ $\mathbf{v}_i$ åŒæ—¶å‡ºç°åœ¨ FM çš„äº¤å‰é¡¹å’Œ Deep çš„éçº¿æ€§è¡¨ç¤ºä¸­ï¼Œ
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_i}
= \underbrace{\frac{\partial \mathcal{L}}{\partial y_{\text{FM}}}\frac{\partial y_{\text{FM}}}{\partial \mathbf{v}_i}}_{\text{ä½é˜¶äº¤å‰ä¿¡å·}}
+ \underbrace{\frac{\partial \mathcal{L}}{\partial z_{\text{deep}}}\frac{\partial z_{\text{deep}}}{\partial \mathbf{v}_i}}_{\text{é«˜é˜¶äº¤å‰ä¿¡å·}},
$$
è¿™ä½¿å¾— Embedding åŒæ—¶è¢«**ä½é˜¶äº¤å‰**å’Œ**é«˜é˜¶éçº¿æ€§äº¤å‰**çš„æ¢¯åº¦å…±åŒâ€œé›•åˆ»â€ï¼Œä»è€Œåœ¨è¯­ä¹‰ç©ºé—´ä¸­å­¦åˆ°æ›´ç»†è…»çš„ç»“æ„ã€‚

![DeepFMæ¨¡å‹ç»“æ„å›¾](/img/ch03/model_DeepFM.png)

::: details ğŸ’» DeepFM æ¨¡å‹å®ç° 
```python
import torch
import torch.nn as nn
from torch_rechub.models.layers import MLP, FM, EmbeddingLayer

class DeepFM(nn.Module):
    """
    DeepFM Model
    
    Args:
        deep_features (list): ç”¨äºDeepå’ŒFMéƒ¨åˆ†çš„ç‰¹å¾åˆ—è¡¨.
        mlp_params (dict): Deepä¾§MLPçš„å‚æ•°.
    """
    def __init__(self, deep_features, mlp_params):
        super().__init__()
        # DeepFMä¸­ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½ç”¨äºDeepå’ŒFMéƒ¨åˆ†
        self.feature_layer = EmbeddingLayer(deep_features)
        
        # FMéƒ¨åˆ†
        self.fm = FM(reduce_sum=True)
        
        # Deepéƒ¨åˆ†
        self.deep_mlp = MLP(self.feature_layer.input_dim, **mlp_params)
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        # 1 (æ¥è‡ªFM) + mlp_output_dim (æ¥è‡ªDeep)
        self.output_layer = nn.Linear(mlp_params["output_dim"] + 1, 1)

    def forward(self, x):
        """
        Args:
            x (dict): è¾“å…¥ç‰¹å¾å­—å…¸.
        """
        # è·å–æ‰€æœ‰ç‰¹å¾çš„Embedding
        embed_x = self.feature_layer(x) # (batch_size, n_fields, embed_dim)
        
        # FMéƒ¨åˆ†è¾“å‡º
        fm_out = self.fm(embed_x) # (batch_size, 1)
        
        # Deepéƒ¨åˆ†è¾“å…¥éœ€è¦å°†Embeddingå±•å¹³
        deep_input = embed_x.flatten(start_dim=1) # (batch_size, n_fields * embed_dim)
        deep_out = self.deep_mlp(deep_input) # (batch_size, mlp_output_dim)
        
        # æ‹¼æ¥FMå’ŒDeepçš„è¾“å‡º
        concat_out = torch.cat([fm_out, deep_out], dim=1)
        
        # æœ€ç»ˆé¢„æµ‹
        y_pred = self.output_layer(concat_out)
        return torch.sigmoid(y_pred)
```
:::

#### DCN V2ï¼šæ›´å¼ºå¤§çš„æ˜¾å¼äº¤å‰ç½‘ç»œ

`DCN` (Deep & Cross Network) æä¾›äº†å¦ä¸€ç§æ˜¾å¼å­¦ä¹ ç‰¹å¾äº¤å‰çš„æ€è·¯ã€‚å®ƒè®¾è®¡çš„`Cross Network`å¯ä»¥åœ¨æ¯ä¸€å±‚éƒ½ä¿ç•™ä¹‹å‰æ‰€æœ‰å±‚çš„äº¤å‰ç»“æœï¼Œå¹¶ä¸åŸå§‹è¾“å…¥è¿›è¡Œæ–°ä¸€è½®çš„äº¤å‰ã€‚`DCN V2` æ˜¯å…¶æ”¹è¿›ç‰ˆï¼Œç»“æ„æ›´ç¨³å®šï¼Œè¡¨ç°æ›´ä¼˜ã€‚

-   **æ ¸å¿ƒæ€æƒ³**ï¼š
    -   **Crosséƒ¨åˆ†**ï¼šç”¨ä¸€ä¸ªç‰¹å®šçš„ç½‘ç»œç»“æ„ï¼Œåœ¨æ¯ä¸€å±‚éƒ½æ˜¾å¼åœ°ã€è‡ªåŠ¨åœ°è¿›è¡Œç‰¹å¾äº¤å‰ï¼Œäº¤å‰çš„é˜¶æ•°ç”±ç½‘ç»œæ·±åº¦å†³å®šã€‚
    -   **Deepéƒ¨åˆ†**ï¼šä¸€ä¸ªæ ‡å‡†çš„DNNï¼Œä¸Crosséƒ¨åˆ†å¹¶è¡Œã€‚
-   **ç²¾é«“**ï¼šç›¸æ¯”äºFMåªèƒ½åšäºŒé˜¶äº¤å‰ï¼ŒCrossç½‘ç»œèƒ½ä»¥ä¸€ç§é«˜æ•ˆä¸”æœ‰ç•Œçš„æ–¹å¼ï¼Œå­¦ä¹ æ›´é«˜é˜¶çš„äº¤å‰ç‰¹å¾ã€‚

#### ğŸ§® æ•°å­¦å»ºæ¨¡ï¼šCross Network çš„æ˜¾å¼å¤šé˜¶äº¤å‰

è®¾è¾“å…¥ä¸ºå±•å¹³åçš„å‘é‡ $\mathbf{x}_0\in\mathbb{R}^d$ï¼ŒCross Network ä¾æ¬¡äº§ç”Ÿ $L_c$ å±‚äº¤å‰è¡¨ç¤º $\mathbf{x}_1,\dots,\mathbf{x}_{L_c}$ã€‚ç»å…¸ DCN çš„ç¬¬ $\ell$ å±‚äº¤å‰å¯å†™ä¸ºï¼š

$$
\mathbf{x}_{\ell+1} = \mathbf{x}_0\, (\mathbf{w}_\ell^\top \mathbf{x}_\ell) + \mathbf{b}_\ell + \mathbf{x}_\ell, \quad \ell=0,\dots,L_c-1,
$$

å…¶ä¸­ $\mathbf{w}_\ell\in\mathbb{R}^d$ã€$\mathbf{b}_\ell\in\mathbb{R}^d$ã€‚

- $\mathbf{w}_\ell^\top \mathbf{x}_\ell$ æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œè¡¨ç¤ºå½“å‰å±‚æ ¹æ® $\mathbf{x}_\ell$ è®¡ç®—å‡ºçš„â€œç¼©æ”¾ç³»æ•°â€ï¼›
- $\mathbf{x}_0\, (\mathbf{w}_\ell^\top \mathbf{x}_\ell)$ è¡¨ç¤ºç”¨è¯¥ç³»æ•°åœ¨åŸå§‹è¾“å…¥æ–¹å‘ä¸Šåšç¼©æ”¾ï¼›
- å†åŠ ä¸Šæ®‹å·® $\mathbf{x}_\ell$ï¼Œå®ç°â€œåœ¨æ—§ç‰¹å¾çš„åŸºç¡€ä¸Šå åŠ ä¸€å±‚æ–°çš„äº¤å‰ç»“æ„â€ã€‚

å±•å¼€æ¥çœ‹ï¼Œ$\mathbf{x}_1$ åŒ…å«ä¸€é˜¶ä¸äºŒé˜¶é¡¹ï¼Œ$\mathbf{x}_2$ å°†ç»§ç»­äº§ç”Ÿä¸‰é˜¶é¡¹â€¦â€¦å †å  $L_c$ å±‚åï¼ŒCross Network å®é™…ä¸Šåœ¨ä¸€ä¸ªå—æ§çš„å‚æ•°é‡å†…ï¼Œæ„é€ äº†ä»ä¸€é˜¶åˆ° $L_c+1$ é˜¶çš„å¤šé¡¹å¼ç‰¹å¾ï¼Œè€Œæ— éœ€æ˜¾å¼æšä¸¾æ‰€æœ‰ç»„åˆã€‚

åœ¨ DCN V2 ä¸­ï¼ŒCross å±‚ä½¿ç”¨äº†æ›´çµæ´»çš„å‚æ•°åŒ–å½¢å¼ï¼ˆå¦‚å‘é‡-å‘é‡ä¹˜æ³•æ›¿ä»£æ ‡é‡ç¼©æ”¾ï¼‰ï¼Œä½†æ€»ä½“æ€æƒ³ä¿æŒä¸å˜ï¼š**æ²¿ç€åŸå§‹è¾“å…¥ $\mathbf{x}_0$ çš„æ–¹å‘ï¼Œé€å±‚æ„é€ é«˜é˜¶äº¤å‰é¡¹å¹¶å åŠ åˆ°å½“å‰è¡¨ç¤ºä¸Š**ã€‚
![DCN V2æ¨¡å‹ç»“æ„å›¾](/img/ch03/model_DeepCross.png)

![DCN V2 Cross Layerç»“æ„å›¾](/img/ch03/model_DeepCross_CrossLayer.png)

::: details ğŸ’» DCN V2 æ¨¡å‹å®ç° 
```python
import torch
import torch.nn as nn
from torch_rechub.models.layers import MLP, CrossNetworkV2, EmbeddingLayer

class DCNv2(nn.Module):
    """
    Deep & Cross Network V2
    
    Args:
        features (list): æ‰€æœ‰ç‰¹å¾åˆ—è¡¨.
        cross_layer_num (int): Crossç½‘ç»œçš„å±‚æ•°.
        mlp_params (dict): Deepä¾§MLPçš„å‚æ•°.
    """
    def __init__(self, features, cross_layer_num, mlp_params):
        super().__init__()
        self.feature_layer = EmbeddingLayer(features)
        self.input_dim = self.feature_layer.input_dim
        
        # Crosséƒ¨åˆ†
        self.cross_net = CrossNetworkV2(self.input_dim, cross_layer_num)
        
        # Deepéƒ¨åˆ†
        self.deep_mlp = MLP(self.input_dim, **mlp_params)
        
        # æ‹¼æ¥Crosså’ŒDeepçš„è¾“å‡º
        self.output_layer = nn.Linear(self.input_dim + mlp_params["output_dim"], 1)

    def forward(self, x):
        embed_x = self.feature_layer(x).flatten(start_dim=1) # (batch_size, n_fields * embed_dim)
        
        # Crosséƒ¨åˆ†è¾“å‡º
        cross_out = self.cross_net(embed_x)
        
        # Deepéƒ¨åˆ†è¾“å‡º
        deep_out = self.deep_mlp(embed_x)
        
        # æ‹¼æ¥
        concat_out = torch.cat([cross_out, deep_out], dim=1)
        
        # æœ€ç»ˆé¢„æµ‹
        y_pred = self.output_layer(concat_out)
        return torch.sigmoid(y_pred)
```
:::

### æ³¨æ„åŠ›æ—¶ä»£ï¼šè®©æ¨¡å‹çŸ¥é“è¯¥"çœ‹"å“ªé‡Œ

ç”¨æˆ·çš„å…´è¶£æ˜¯å¤šæ ·ä¸”åŠ¨æ€çš„ã€‚æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ï¼Œè®©æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°èšç„¦äºä¸å½“å‰å€™é€‰ç‰©å“æœ€ç›¸å…³çš„å†å²è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„å…´è¶£å»ºæ¨¡ã€‚

å…³äºæ³¨æ„åŠ›æœºåˆ¶åœ¨æ¨èç³»ç»Ÿä¸­çš„æ·±å…¥åº”ç”¨ï¼ˆåŒ…æ‹¬ DINã€DIENã€BST ç­‰æ¨¡å‹çš„æ•°å­¦åŸç†ã€å®ç°ç»†èŠ‚ä¸å·¥ç¨‹ä¼˜åŒ–ï¼‰ï¼Œè¯·å‚è§ä¸‹ä¸€ç« èŠ‚ï¼š[æ³¨æ„åŠ›æ¨¡å‹ï¼šè®©æ¨èæ¨¡å‹"çœ‹"åˆ°ä½ çš„å…´è¶£ç„¦ç‚¹](./6.attention_models.md).


### å¤šç›®æ ‡æ—¶ä»£ï¼šä¸€ä¸ªæ¨¡å‹ï¼Œå¤šç§ä½¿å‘½

åœ¨çœŸå®çš„ä¸šåŠ¡ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€ä¸åªå…³å¿ƒ"ç‚¹å‡»ç‡"ï¼Œè¿˜å…³å¿ƒ"è½¬åŒ–ç‡"ã€"è§‚çœ‹æ—¶é•¿"ã€"åˆ†äº«ç‡"ç­‰å¤šä¸ªæŒ‡æ ‡ã€‚**å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-Task Learning, MTLï¼‰** å°±æ˜¯ä¸ºäº†åŒæ—¶ä¼˜åŒ–è¿™äº›ç›®æ ‡è€Œç”Ÿã€‚

#### MMoEï¼šå·§å¦™å¹³è¡¡å¤šä¸ªä»»åŠ¡çš„"ä¸“å®¶ç½‘ç»œ"

`MMoE` (Multi-gate Mixture-of-Experts) æ˜¯Googleæå‡ºçš„ç»å…¸å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€‚

-   **æ ¸å¿ƒæ€æƒ³**ï¼šä¸åŒçš„ä»»åŠ¡ä¹‹é—´ï¼Œæ—¢æœ‰å…±æ€§ï¼Œä¹Ÿæœ‰ç‰¹æ€§ã€‚å¼ºè¡Œè®©æ‰€æœ‰ä»»åŠ¡å…±äº«ä¸€ä¸ªåº•å±‚ç½‘ç»œï¼ˆShared-Bottomï¼‰å¯èƒ½ä¼šå¯¼è‡´"è··è··æ¿æ•ˆåº”"ï¼ˆä¸€ä¸ªä»»åŠ¡æå‡ï¼Œå¦ä¸€ä¸ªä»»åŠ¡ä¸‹é™ï¼‰ã€‚
-   **ç²¾é«“**ï¼š
    1.  è®¾ç½®å¤šä¸ª**ä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰**ï¼Œæ¯ä¸ªä¸“å®¶éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å­ç½‘ç»œï¼Œå¯ä»¥å­¦ä¹ åˆ°æŸæ–¹é¢çš„å…±åŒçŸ¥è¯†ã€‚
    2.  ä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®ä¸€ä¸ª**é—¨æ§ç½‘ç»œï¼ˆGateï¼‰**ã€‚è¿™ä¸ªé—¨æ§ç½‘ç»œä¼šæ ¹æ®å½“å‰è¾“å…¥ï¼ŒåŠ¨æ€åœ°ä¸ºæ‰€æœ‰ä¸“å®¶åˆ†é…æƒé‡ã€‚
    3.  æ¯ä¸ªä»»åŠ¡çš„æœ€ç»ˆè¾“å…¥ï¼Œæ˜¯æ‰€æœ‰ä¸“å®¶è¾“å‡ºçš„åŠ æƒå’Œã€‚è¿™æ ·ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½å¯ä»¥æŒ‰éœ€ã€çµæ´»åœ°ç»„åˆä¸“å®¶çš„èƒ½åŠ›ï¼Œæ—¢èƒ½å…±äº«ä¿¡æ¯ï¼Œåˆèƒ½ä¿ç•™ä¸ªæ€§ï¼Œæœ‰æ•ˆé¿å…äº†è´Ÿè¿ç§»ã€‚

##### ğŸ§® æ•°å­¦å»ºæ¨¡ï¼šä¸“å®¶æ··åˆä¸å¤šä»»åŠ¡æŸå¤±

è®¾æœ‰ $K$ ä¸ªä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶æ¥æ”¶ç›¸åŒçš„è¾“å…¥è¡¨ç¤º $\mathbf{h}$ï¼Œè¾“å‡ºä¸º $\mathbf{e}_k\in\mathbb{R}^{d_e}$ï¼š
$$
\mathbf{e}_k = f_k(\mathbf{h};\theta_k),\quad k=1,\dots,K.
$$

å¯¹ç¬¬ $t$ ä¸ªä»»åŠ¡ï¼Œé—¨æ§ç½‘ç»œæ ¹æ®è¾“å…¥ $\mathbf{h}$ ç”Ÿæˆå¯¹å„ä¸“å®¶çš„æƒé‡åˆ†å¸ƒï¼š
$$
\mathbf{g}^{(t)} = \text{softmax}(W^{(t)}\mathbf{h})\in\mathbb{R}^K,
$$
å…¶ä¸­ $g^{(t)}_k$ è¡¨ç¤ºä»»åŠ¡ $t$ å¯¹ä¸“å®¶ $k$ çš„ä¾èµ–å¼ºåº¦ï¼Œä¸” $\sum_k g^{(t)}_k=1$ã€‚

ä»»åŠ¡ $t$ çš„æ··åˆä¸“å®¶è¡¨ç¤ºä¸ºï¼š
$$
\mathbf{z}^{(t)} = \sum_{k=1}^K g^{(t)}_k\,\mathbf{e}_k,
$$
éšåé€å…¥è¯¥ä»»åŠ¡çš„å¡”ç½‘ç»œ $f^{(t)}_{\text{tower}}$ï¼Œå¾—åˆ° logit $z^{(t)}$ã€æ¦‚ç‡ $p^{(t)}$ ä¸æŸå¤± $\mathcal{L}_t$ï¼ˆå¦‚äºŒåˆ†ç±»äº¤å‰ç†µï¼‰ï¼š
$$
 z^{(t)} = f^{(t)}_{\text{tower}}(\mathbf{z}^{(t)}),\quad
 p^{(t)} = \sigma(z^{(t)}),\quad
 \mathcal{L}_t = \text{BCE}(y^{(t)},p^{(t)}).
$$

å¤šä»»åŠ¡çš„æ€»ä½“æŸå¤±å¸¸å†™ä¸ºåŠ æƒå’Œï¼š
$$
 \mathcal{L}_{\text{total}} = \sum_{t=1}^T \lambda_t\,\mathcal{L}_t,
$$
å…¶ä¸­ $\lambda_t$ æ§åˆ¶å„ä»»åŠ¡çš„é‡è¦ç¨‹åº¦ã€‚ç”±äºæ¢¯åº¦ä¼šé€šè¿‡ $g^{(t)}_k$ å›æµåˆ°é—¨æ§ç½‘ç»œä¸ä¸“å®¶ç½‘ç»œï¼ŒMMoE èƒ½å¤Ÿåœ¨è®­ç»ƒä¸­**è‡ªåŠ¨å­¦ä¼š**ï¼šå“ªäº›ä¸“å®¶æ›´é€‚åˆè¢«å“ªäº›ä»»åŠ¡â€œé’çâ€ï¼Œä»è€Œç¼“è§£ä»»åŠ¡é—´çš„å†²çªä¸è´Ÿè¿ç§»ã€‚

![MMoEæ¨¡å‹ç»“æ„å›¾](/img/ch03/model_MMoE.png)

::: details ğŸ’» MMoE æ¨¡å‹å®ç° 
```python
import torch
import torch.nn as nn
from torch_rechub.models.layers import MLP, EmbeddingLayer

class MMoE(nn.Module):
    """
    Multi-gate Mixture-of-Experts Model
    
    Args:
        features (list): æ‰€æœ‰è¾“å…¥ç‰¹å¾åˆ—è¡¨.
        task_types (list): ä»»åŠ¡ç±»å‹åˆ—è¡¨, e.g., ['binary', 'binary'].
        n_expert (int): ä¸“å®¶æ•°é‡.
        expert_params (dict): æ¯ä¸ªä¸“å®¶ç½‘ç»œçš„MLPå‚æ•°.
        tower_params_list (list): æ¯ä¸ªä»»åŠ¡å¡”çš„MLPå‚æ•°åˆ—è¡¨.
    """
    def __init__(self, features, task_types, n_expert, expert_params, tower_params_list):
        super().__init__()
        self.feature_layer = EmbeddingLayer(features)
        self.input_dim = self.feature_layer.input_dim
        self.n_task = len(task_types)
        
        # ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            MLP(self.input_dim, **expert_params) for _ in range(n_expert)
        ])
        
        # é—¨æ§ç½‘ç»œ
        self.gates = nn.ModuleList([
            nn.Linear(self.input_dim, n_expert, bias=False) for _ in range(self.n_task)
        ])
        
        # ä»»åŠ¡å¡”
        self.towers = nn.ModuleList([
            MLP(expert_params["output_dim"], **tower_params) for tower_params in tower_params_list
        ])
        
    def forward(self, x):
        input_emb = self.feature_layer(x).flatten(start_dim=1)
        
        # ä¸“å®¶è¾“å‡º
        expert_outputs = [expert(input_emb) for expert in self.experts]
        expert_outputs = torch.stack(expert_outputs, dim=1) # (batch, n_expert, expert_dim)
        
        # é—¨æ§å’Œä»»åŠ¡å¡”è®¡ç®—
        outputs = []
        for i in range(self.n_task):
            gate_value = self.gates[i](input_emb).softmax(dim=1).unsqueeze(-1) # (batch, n_expert, 1)
            
            # é—¨æ§åŠ æƒ
            task_input = torch.sum(expert_outputs * gate_value, dim=1) # (batch, expert_dim)
            
            # ä»»åŠ¡å¡”è¾“å‡º
            task_output = self.towers[i](task_input)
            outputs.append(task_output)
            
        return outputs
```
:::


## ğŸ”‘ å…³é”®è¶…å‚æ•°ä¸è°ƒå‚å¿ƒæ³•

åœ¨æ’åºé˜¶æ®µï¼Œå¾ˆå¤šâ€œç‚¼ä¸¹å‚æ•°â€å…¶å®éƒ½èƒ½ä»å‰æ–‡çš„æ•°å­¦å…¬å¼ä¸­æ‰¾åˆ°ç›´è§‰ä¾æ®ã€‚

1. **Embedding ç»´åº¦ $d$**
   - è¿‡å°ï¼š$\mathbf{e}$ å®¹é‡ä¸è¶³ï¼Œéš¾ä»¥åŒæ—¶æ‰¿è½½ FM/Deep/Attention/GRU ç­‰å„è·¯ä¿¡å·ï¼Œè¡¨ç°ä¸ºè®­ç»ƒæŸå¤±é™ä½ç¼“æ…¢ã€å¬å›/æ’åºéƒ½â€œå¡åœ¨ä¸€ä¸ªå°é˜¶ä¸Šâ€ã€‚
   - è¿‡å¤§ï¼š$\|\mathbf{e}\|_2$ å¾€å¾€å¢å¤§ï¼Œä½¿å¾— logit $z$ çš„æ–¹å·®å˜å¤§ï¼ŒBCE/Focal Loss çš„æ¢¯åº¦ä¹Ÿéšä¹‹æ”¾å¤§ï¼Œæ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œæ¨ç†æ—¶å†…å­˜ä¸ QPS å‹åŠ›ä¹Ÿæ˜æ˜¾å‡é«˜ã€‚
   - **ç»éªŒ**ï¼š
     - ä¸­å°è§„æ¨¡ä¸šåŠ¡ï¼š$d\in[8,32]$ï¼›
     - å¤§è§„æ¨¡ã€é«˜ç»´ç¨€ç–ä¸šåŠ¡ï¼š$d\in[16,64]$ï¼Œå¹¶é…åˆ Dropout/L2 æ­£åˆ™ã€‚
   - **ä¸å¬å› Embedding çš„å…³ç³»**ï¼šå¸¸è§åšæ³•æ˜¯**ç‰©å“ä¾§å¤ç”¨ã€ç”¨æˆ·ä¾§åŒºåˆ†**ï¼šç‰©å“ Embedding å¯ä»¥å…±äº«å¬å›å¡”çš„å‘é‡ï¼ˆä¿è¯â€œåŒä¸€ç‰©å“åœ¨ä¸åŒé˜¶æ®µè¯­ä¹‰ä¸€è‡´â€ï¼‰ï¼Œæ’åºä¾§å†åœ¨æ­¤åŸºç¡€ä¸Šå åŠ æ›´å¤šä¸Šä¸‹æ–‡ç‰¹å¾ï¼›è€Œç”¨æˆ·å…´è¶£åœ¨å¬å›/æ’åºä¸­çš„ä¾§é‡ç‚¹ä¸åŒï¼Œé€šå¸¸ä½¿ç”¨ç‹¬ç«‹å¡”æ›´çµæ´»ã€‚

2. **DNN æ·±åº¦ä¸å®½åº¦ï¼ˆ$L$ ä¸æ¯å±‚ç»´åº¦ï¼‰**
   - åœ¨ Wide&Deepã€DeepFMã€DCN çš„ Deep éƒ¨åˆ†ä¸­ï¼Œ$L$ å’Œéšè—ç»´åº¦è¶Šå¤§ï¼Œ$\mathbf{h}^{(L)}$ å¯¹è¾“å…¥çš„é«˜é˜¶éçº¿æ€§ç»„åˆè¶Šä¸°å¯Œï¼Œä½†ä¹Ÿæ›´å®¹æ˜“å‡ºç°æ¢¯åº¦ä¸ç¨³å®šä¸è¿‡æ‹Ÿåˆã€‚
   - ä»æ¢¯åº¦è§’åº¦çœ‹ï¼Œå½“ $L$ å¢å¤§æ—¶ï¼Œ$\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}}$ è¦é€šè¿‡æ›´å¤šé“¾å¼ä¹˜ç§¯ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼Œéœ€è¦å€ŸåŠ© **æ®‹å·®ã€å½’ä¸€åŒ–ã€åˆç†åˆå§‹åŒ–** æ¥ç¨³å®šè®­ç»ƒã€‚
   - **ç»éªŒ**ï¼šæ’åºé˜¶æ®µå¸¸ç”¨ 2â€“4 å±‚ MLPï¼Œæ¯å±‚ 128â€“512 ç»´å³å¯ï¼›å¦‚æœç‰¹å¾æå¤šï¼Œå¯ä¼˜å…ˆåŠ å®½è€Œä¸æ˜¯ä¸€å‘³åŠ æ·±ã€‚

3. **æ ·æœ¬ä¸å¹³è¡¡ï¼šåŠ æƒ BCE / Focal Loss / é‡‡æ ·ç­–ç•¥**
   - å½“æ­£è´Ÿæ ·æœ¬æä¸å¹³è¡¡æ—¶ï¼ŒBCE æ¢¯åº¦ $p-y$ åœ¨ç»å¤§å¤šæ•°è´Ÿæ ·æœ¬ä¸Šæ¥è¿‘ 0 è€Œåœ¨å°‘æ•°æ­£æ ·æœ¬ä¸Šåå¤§ï¼Œå¯èƒ½å‡ºç°â€œæ¨¡å‹æ€»æ˜¯é¢„æµ‹ 0 ä¹Ÿä¸é”™â€çš„å±€é¢ã€‚
   - **åŠ æƒ BCE** ä¸­çš„ $w_1,w_0$ å®è´¨ä¸Šåœ¨æŸå¤±ä¸­æ”¾å¤§äº†æŸç±»æ ·æœ¬çš„æ¢¯åº¦è´¡çŒ®ï¼›**Focal Loss** ä¸­çš„ $(1-p)^\gamma,p^\gamma$ åˆ™ä¼šè‡ªåŠ¨å‡å°‘â€œå·²ç»å­¦ä¼šçš„ç®€å•æ ·æœ¬â€çš„æƒé‡ã€‚
   - å®æˆ˜ä¸­ï¼Œå¯ä»¥å°†ï¼š
     - ç±»åˆ«æƒé‡ $w_1:w_0$ æˆ– Focal çš„ $\alpha$ è®¾ç½®ä¸ºè¿‘ä¼¼äºè´Ÿæ ·æœ¬:æ­£æ ·æœ¬æ¯”ä¾‹çš„å€’æ•°ï¼›
     - å†é€šè¿‡è°ƒèŠ‚ $\gamma$ æ§åˆ¶å¯¹â€œéš¾æ ·æœ¬â€çš„èšç„¦ç¨‹åº¦ï¼ˆå¦‚ 1â€“3ï¼‰ã€‚

6. **å¤šä»»åŠ¡æƒé‡ $\lambda_t$**
   - åœ¨ MMoE ä¸­ï¼Œæ€»æŸå¤± $\mathcal{L}_{\text{total}}=\sum_t\lambda_t\mathcal{L}_t$ ä¸­çš„ $\lambda_t$ å†³å®šäº†æ¢¯åº¦åœ¨ä¸åŒä»»åŠ¡é—´å¦‚ä½•åˆ†é…ï¼š
     - $\lambda_{\text{ä¸»ä»»åŠ¡}}$ è¿‡å°ï¼šä¸»ä»»åŠ¡è¢«â€œé™ªè·‘ä»»åŠ¡â€æ‹–ç´¯ï¼›
     - $\lambda_{\text{ä¸»ä»»åŠ¡}}$ è¿‡å¤§ï¼šè¾…åŠ©ä»»åŠ¡å‡ ä¹ä¸èµ·ä½œç”¨ï¼Œé€€åŒ–ä¸ºå•ä»»åŠ¡ã€‚
   - å¸¸è§åšæ³•åŒ…æ‹¬ï¼šäººå·¥è®¾å®šï¼ˆå¦‚ä¸»ä»»åŠ¡æƒé‡å¤§ä¸€äº›ï¼‰ã€ä¾æ®æŸå¤±å°ºåº¦è‡ªé€‚åº”ç¼©æ”¾ã€æˆ–ä½¿ç”¨ä¸ç¡®å®šæ€§åŠ æƒç­‰è‡ªåŠ¨æ–¹æ³•ã€‚

æ•´ä½“æ€è·¯æ˜¯ï¼š**å…ˆç”¨å°æ¨¡å‹ + ç®€å•é…ç½®å¿«é€Ÿè·‘é€šï¼Œå†å›´ç»•ä¸Šè¿°å…³é”®è¶…å‚åšå°æ­¥æœç´¢**ï¼Œæ¯ä¸€æ­¥è°ƒå‚éƒ½èƒ½åœ¨å…¬å¼å±‚é¢æ‰¾åˆ°â€œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåŠ¨æ‰‹è„šâ€çš„ç†ç”±ã€‚

## ğŸ“– **å»¶ä¼¸é˜…è¯»**
1. [Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792) - Wide&Deepçš„å¼€å±±ä¹‹ä½œã€‚
2. [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/abs/1703.04247) - DeepFMçš„ç»å…¸è®ºæ–‡ã€‚
3. [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/10.1145/3219819.3220007) - MMoEçš„ç»å…¸è®ºæ–‡ï¼Œå¤šä»»åŠ¡å­¦ä¹ çš„å¿…è¯»æ–‡çŒ®ã€‚
4. [Torch-RecHub Ranking Models](https://github.com/datawhalechina/torch-rechub/tree/main/torch_rechub/models/ranking) - Datawhaleå¼€æºçš„torch-rechubé¡¹ç›®ä¸­ï¼ŒåŒ…å«äº†å¤šç§ç»å…¸æ’åºæ¨¡å‹çš„PyTorchå®ç°ã€‚
5. å…³äºæ³¨æ„åŠ›æ¨¡å‹ï¼ˆDIN/DIEN/BST ç­‰ï¼‰çš„æ•°å­¦åŸç†ä¸å·¥ç¨‹å®ç°ï¼Œæ¨èé˜…è¯»ä¸‹ä¸€èŠ‚æ–‡æ¡£ï¼š[æ³¨æ„åŠ›é©¬è¾¾ï¼šè®©æ¨èæ¨¡å‹"çœ‹"åˆ°ä½ çš„å…´è¶£ç„¦ç‚¹](./6.attention_models.md)ã€‚

> ğŸ§  **æ€è€ƒé¢˜**
>
> 1. DeepFMçš„Embeddingå…±äº«æœºåˆ¶æ˜¯å…¶äº®ç‚¹ï¼Œä½†ä¹Ÿå¯èƒ½å¸¦æ¥å‚æ•°æ›´æ–°çš„å†²çªã€‚ä½ è®¤ä¸ºåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ï¼ŒFMå’ŒDeepéƒ¨åˆ†ä½¿ç”¨ç‹¬ç«‹çš„Embeddingå¯èƒ½ä¼šæ›´å¥½ï¼Ÿ
>
> 2. DCNå’ŒxDeepFMéƒ½è‡´åŠ›äºæ˜¾å¼åœ°å­¦ä¹ é«˜é˜¶ç‰¹å¾äº¤å‰ï¼Œå®ƒä»¬çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿåœ¨è®¡ç®—æ•ˆç‡å’Œæ•ˆæœä¸Šå„æœ‰ä»€ä¹ˆå–èˆï¼Ÿ
>
> 3. åœ¨çº¯æ’åºæ¨¡å‹ï¼ˆå¦‚Wide&Deepã€DeepFMã€DCNï¼‰ä¹‹å¤–æ¥å…¥æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå¦‚DIN/DIEN/BSTï¼‰ï¼Œä½ è®¤ä¸ºåœ¨æ•´ä½“æ¶æ„ä¸Šåº”å¦‚ä½•åˆ’åˆ†èŒè´£è¾¹ç•Œï¼Ÿ
>
> 4. MMoEä¸­çš„"ä¸“å®¶"æ•°é‡æ˜¯ä¸€ä¸ªè¶…å‚ï¼Œä½ è®¤ä¸ºå®ƒåº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿæ˜¯è¶Šå¤šè¶Šå¥½å—ï¼Ÿå®ƒå’Œä»»åŠ¡çš„ç›¸å…³æ€§æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

::: tip ğŸ‰ ç« èŠ‚å°ç»“
æ·±åº¦æ’åºæ¨¡å‹çš„æ¼”è¿›å†ç¨‹å±•ç°äº†æ¨èç³»ç»Ÿä»"æ‰‹å·¥ç‚¼ä¸¹"åˆ°"è‡ªåŠ¨é©¾é©¶"çš„åä¸½è½¬èº«ã€‚Wide&Deepå¥ å®šäº†è®°å¿†ä¸æ³›åŒ–å¹¶é‡çš„åŸºç¡€èŒƒå¼ï¼ŒDeepFMå®ç°äº†ç‰¹å¾äº¤å‰çš„è‡ªåŠ¨åŒ–çªç ´ï¼ŒDINå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ•è·åŠ¨æ€å…´è¶£ï¼ŒMMoEå°†å•ä¸€ç›®æ ‡æ‰©å±•ä¸ºå¤šä»»åŠ¡ååŒä¼˜åŒ–ã€‚æ¯ä¸€æ¬¡æ¨¡å‹åˆ›æ–°ï¼Œéƒ½æ˜¯å¯¹ç”¨æˆ·å¤æ‚åå¥½çš„æ›´æ·±å±‚ç†è§£ï¼Œä¹Ÿæ˜¯å¯¹æ¨èç²¾åº¦æé™çš„ä¸æ‡ˆè¿½æ±‚ã€‚æŒæ¡è¿™äº›æ¨¡å‹çš„ç²¾é«“ï¼Œå°±æ˜¯æŒæ¡äº†åœ¨æ¨èç³»ç»Ÿ"æœ€åä¸€å…¬é‡Œ"ä¸­ç²¾é›•ç»†ç¢çš„è‰ºæœ¯ã€‚
:::

---

> "The devil is in the details." â€”â€” Ludwig Mies van der Rohe  
> åœ¨æ’åºæ¨¡å‹çš„ä¸–ç•Œé‡Œï¼Œé­”é¬¼è—åœ¨ç»†èŠ‚ä¸­ï¼Œè€Œå¤©ä½¿ä¹Ÿåœ¨ç»†èŠ‚ä¸­è¯ç”Ÿã€‚

