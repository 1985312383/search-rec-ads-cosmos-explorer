---
title: æ·±åº¦å­¦ä¹ åœ¨æ¨èä¸­çš„'æ–‡è‰ºå¤å…´'
createTime: 2025/06/05 09:34:56
---

# æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿï¼šé©å‘½æ€§çš„çªç ´

2016å¹´ï¼ŒGoogleå‘å¸ƒäº†Wide & Deepæ¨¡å‹ï¼Œæ ‡å¿—ç€æ·±åº¦å­¦ä¹ åœ¨æ¨èç³»ç»Ÿé¢†åŸŸçš„æ­£å¼å´›èµ·ã€‚ä»æ­¤ï¼Œæ¨èç³»ç»Ÿè¿›å…¥äº†"æ·±åº¦å­¦ä¹ æ—¶ä»£"â€”â€”ä¸å†éœ€è¦ç¹é‡çš„ç‰¹å¾å·¥ç¨‹ï¼Œä¸å†å±€é™äºçº¿æ€§å‡è®¾ï¼Œè€Œæ˜¯è®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ å¤æ‚çš„ç”¨æˆ·-ç‰©å“äº¤äº’æ¨¡å¼ã€‚

## ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ èƒ½é©å‘½æ¨èç³»ç»Ÿï¼Ÿ ğŸš€

### ä¼ ç»Ÿæ¨èçš„å±€é™æ€§

**çº¿æ€§æ¨¡å‹çš„å›°å¢ƒï¼š**
- ååŒè¿‡æ»¤ï¼š$\hat{r}_{ui} = \mu + b_u + b_i + \mathbf{p}_u^T \mathbf{q}_i$
- åªèƒ½æ•è·çº¿æ€§å…³ç³»ï¼Œæ— æ³•å¤„ç†å¤æ‚äº¤äº’

**ç‰¹å¾å·¥ç¨‹çš„ç—›ç‚¹ï¼š**
- éœ€è¦äººå·¥è®¾è®¡ç‰¹å¾ç»„åˆï¼šageÃ—gender, categoryÃ—price
- ç»„åˆçˆ†ç‚¸ï¼š$n$ ä¸ªç‰¹å¾æœ‰ $2^n$ ç§ç»„åˆå¯èƒ½
- éš¾ä»¥å‘ç°éšè—æ¨¡å¼

### æ·±åº¦å­¦ä¹ çš„çªç ´

**éçº¿æ€§å»ºæ¨¡èƒ½åŠ›ï¼š**
$$f(x) = \sigma(W_3 \cdot \sigma(W_2 \cdot \sigma(W_1 \cdot x + b_1) + b_2) + b_3)$$

å…¶ä¸­ $\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ï¼Œä½¿æ¨¡å‹èƒ½å­¦ä¹ ä»»æ„å¤æ‚çš„éçº¿æ€§æ˜ å°„ã€‚

**è‡ªåŠ¨ç‰¹å¾å­¦ä¹ æ¶æ„ï¼š**
```
è¾“å…¥ç‰¹å¾ â†’ åµŒå…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡º
   â†“         â†“        â†“         â†“              â†“
åŸå§‹ç‰¹å¾   ç¨ å¯†è¡¨ç¤º   åˆçº§ç‰¹å¾   é«˜çº§ç‰¹å¾      æœ€ç»ˆé¢„æµ‹
```

## æ ¸å¿ƒç®—æ³•è¯¦è§£ ğŸ¯

### 1. Wide & Deepï¼šè®°å¿†ä¸æ³›åŒ–çš„å¹³è¡¡

**æ ¸å¿ƒæ€æƒ³ï¼š**
- Wideéƒ¨åˆ†ï¼šè®°å¿†å†å²å…±ç°æ¨¡å¼ 
- Deepéƒ¨åˆ†ï¼šæ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰¹å¾ç»„åˆ

**æ•°å­¦è¡¨è¾¾ï¼š**
- Wide: $y_{wide} = \mathbf{w}^T \mathbf{x} + b$
- Deep: $y_{deep} = \mathbf{W}_L^T \mathbf{a}_L + b_L$
- æœ€ç»ˆ: $P(Y=1|\mathbf{x}) = \sigma(y_{wide} + y_{deep})$

**å®Œæ•´å®ç°ï¼š**

```python
import torch
import torch.nn as nn

class WideAndDeep(nn.Module):
    def __init__(self, wide_dim, embed_dims, deep_dims, dropout=0.2):
        super(WideAndDeep, self).__init__()
        
        # Wideéƒ¨åˆ†ï¼šçº¿æ€§å±‚å¤„ç†ç¨€ç–ç‰¹å¾
        self.wide = nn.Linear(wide_dim, 1)
        
        # Deepéƒ¨åˆ†ï¼šåµŒå…¥å±‚å¤„ç†ç±»åˆ«ç‰¹å¾
        self.embeddings = nn.ModuleList([
            nn.Embedding(vocab_size, 32) for vocab_size in embed_dims
        ])
        
        # Deepéƒ¨åˆ†ï¼šå…¨è¿æ¥ç½‘ç»œ
        deep_input_dim = len(embed_dims) * 32
        layers = []
        for i, dim in enumerate(deep_dims):
            if i == 0:
                layers.extend([
                    nn.Linear(deep_input_dim, dim),
                    nn.ReLU(),
                    nn.Dropout(dropout)
                ])
            else:
                layers.extend([
                    nn.Linear(deep_dims[i-1], dim),
                    nn.ReLU(), 
                    nn.Dropout(dropout)
                ])
        
        layers.append(nn.Linear(deep_dims[-1], 1))
        self.deep = nn.Sequential(*layers)
        
    def forward(self, wide_features, deep_features):
        # Wideéƒ¨åˆ†å‰å‘ä¼ æ’­
        wide_out = self.wide(wide_features.float())
        
        # Deepéƒ¨åˆ†åµŒå…¥
        deep_embeds = []
        for i, embedding in enumerate(self.embeddings):
            deep_embeds.append(embedding(deep_features[:, i]))
        
        # æ‹¼æ¥æ‰€æœ‰åµŒå…¥
        deep_input = torch.cat(deep_embeds, dim=1)
        deep_out = self.deep(deep_input)
        
        # åˆå¹¶Wideå’ŒDeep
        logits = wide_out + deep_out
        return torch.sigmoid(logits)

# è®­ç»ƒå‡½æ•°
def train_wide_deep(model, train_loader, epochs=10, lr=0.001):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for wide_feats, deep_feats, labels in train_loader:
            optimizer.zero_grad()
            
            predictions = model(wide_feats, deep_feats)
            loss = criterion(predictions.squeeze(), labels.float())
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')
```

**å®è·µè¦ç‚¹ï¼š**
1. **ç‰¹å¾åˆ†é…ç­–ç•¥**ï¼šç¨€ç–äº¤å‰ç‰¹å¾ç»™Wideï¼Œå¯†é›†ç‰¹å¾ç»™Deep
2. **æ­£åˆ™åŒ–æŠ€å·§**ï¼šWideç”¨L1ï¼ŒDeepç”¨Dropout
3. **å­¦ä¹ ç‡è°ƒä¼˜**ï¼šWideå’ŒDeepå¯ä»¥ç”¨ä¸åŒå­¦ä¹ ç‡

### 2. DeepFMï¼šè‡ªåŠ¨ç‰¹å¾äº¤å‰çš„é‡Œç¨‹ç¢‘

**çªç ´æ€§åˆ›æ–°ï¼š**
- æ— éœ€äººå·¥ç‰¹å¾å·¥ç¨‹
- FMå’ŒDNNå…±äº«embeddingå‚æ•°
- è‡ªåŠ¨å­¦ä¹ ä½é˜¶å’Œé«˜é˜¶ç‰¹å¾äº¤å‰

**ç®—æ³•ç»„ä»¶ï¼š**

| ç»„ä»¶ | æ•°å­¦è¡¨è¾¾ | ä½œç”¨ |
|------|----------|------|
| ä¸€é˜¶é¡¹ | $\sum_{i=1}^n w_i x_i$ | çº¿æ€§æ•ˆåº” |
| äºŒé˜¶é¡¹(FM) | $\sum_{i=1}^n \sum_{j=i+1}^n \langle\mathbf{v}_i, \mathbf{v}_j\rangle x_i x_j$ | äºŒé˜¶äº¤å‰ |
| é«˜é˜¶é¡¹(DNN) | $DNN(\mathbf{e}_1, \mathbf{e}_2, ..., \mathbf{e}_n)$ | é«˜é˜¶äº¤å‰ |

**æ ¸å¿ƒå®ç°ï¼š**

```python
class DeepFM(nn.Module):
    def __init__(self, field_dims, embed_dim=16, deep_dims=[256, 128], dropout=0.2):
        super(DeepFM, self).__init__()
        
        # åµŒå…¥å±‚ï¼ˆFMå’ŒDNNå…±äº«ï¼‰
        self.embeddings = nn.ModuleList([
            nn.Embedding(field_dim, embed_dim) for field_dim in field_dims
        ])
        
        # ä¸€é˜¶æƒé‡
        self.linear_weights = nn.ModuleList([
            nn.Embedding(field_dim, 1) for field_dim in field_dims
        ])
        
        # DNNéƒ¨åˆ†
        deep_input_dim = len(field_dims) * embed_dim
        dnn_layers = []
        for i, dim in enumerate(deep_dims):
            if i == 0:
                dnn_layers.extend([
                    nn.Linear(deep_input_dim, dim),
                    nn.BatchNorm1d(dim),
                    nn.ReLU(),
                    nn.Dropout(dropout)
                ])
            else:
                dnn_layers.extend([
                    nn.Linear(deep_dims[i-1], dim),
                    nn.BatchNorm1d(dim), 
                    nn.ReLU(),
                    nn.Dropout(dropout)
                ])
        
        dnn_layers.append(nn.Linear(deep_dims[-1], 1))
        self.dnn = nn.Sequential(*dnn_layers)
        
        # åç½®é¡¹
        self.bias = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        # è·å–åµŒå…¥å‘é‡
        embeds = []
        linear_terms = []
        
        for i, (embed_layer, linear_layer) in enumerate(zip(self.embeddings, self.linear_weights)):
            embed = embed_layer(x[:, i])
            linear = linear_layer(x[:, i])
            
            embeds.append(embed)
            linear_terms.append(linear)
        
        # ä¸€é˜¶é¡¹
        linear_output = torch.sum(torch.cat(linear_terms, dim=1), dim=1) + self.bias
        
        # äºŒé˜¶äº¤å‰é¡¹ï¼ˆFMéƒ¨åˆ†ï¼‰
        embed_stack = torch.stack(embeds, dim=1)  # [batch_size, field_num, embed_dim]
        
        # FMäº¤å‰ï¼š(sum of embeds)^2 - sum of (embeds^2)
        square_of_sum = torch.sum(embed_stack, dim=1) ** 2
        sum_of_square = torch.sum(embed_stack ** 2, dim=1)
        fm_output = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1)
        
        # DNNéƒ¨åˆ†
        dnn_input = torch.cat(embeds, dim=1)
        dnn_output = self.dnn(dnn_input).squeeze(1)
        
        # æœ€ç»ˆè¾“å‡º
        output = linear_output + fm_output + dnn_output
        return torch.sigmoid(output)
```

### 3. DINï¼šä¸ªæ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶

**æ ¸å¿ƒæ´å¯Ÿï¼š**
ç”¨æˆ·å…´è¶£æ˜¯å¤šæ ·åŒ–çš„ï¼Œåº”è¯¥æ ¹æ®å€™é€‰ç‰©å“åŠ¨æ€å…³æ³¨å†å²è¡Œä¸º

**æ³¨æ„åŠ›å…¬å¼ï¼š**
$$\alpha_i = \frac{\exp(f(\mathbf{h}_i, \mathbf{c}))}{\sum_{j=1}^{T} \exp(f(\mathbf{h}_j, \mathbf{c}))}$$

å…¶ä¸­ï¼š
- $\mathbf{h}_i$ï¼šç¬¬iä¸ªå†å²è¡Œä¸ºåµŒå…¥
- $\mathbf{c}$ï¼šå€™é€‰ç‰©å“åµŒå…¥  
- $f(\cdot, \cdot)$ï¼šæ³¨æ„åŠ›å‡½æ•°

**æ³¨æ„åŠ›å®ç°ï¼š**

```python
class AttentionLayer(nn.Module):
    def __init__(self, embed_dim, hidden_dim=256):
        super(AttentionLayer, self).__init__()
        
        # æ³¨æ„åŠ›ç½‘ç»œï¼š4ç§äº¤äº’ç‰¹å¾
        self.attention_net = nn.Sequential(
            nn.Linear(embed_dim * 4, hidden_dim),  # æ‹¼æ¥ã€ä¹˜ç§¯ã€å·®å€¼ã€å€™é€‰
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(), 
            nn.Linear(hidden_dim // 2, 1)
        )
        
    def forward(self, candidate_embed, history_embeds, mask=None):
        """
        candidate_embed: [batch_size, embed_dim] å€™é€‰ç‰©å“
        history_embeds: [batch_size, seq_len, embed_dim] å†å²è¡Œä¸º
        mask: [batch_size, seq_len] æœ‰æ•ˆè¡Œä¸ºmask
        """
        batch_size, seq_len, embed_dim = history_embeds.shape
        
        # å€™é€‰åµŒå…¥æ‰©å±•åˆ°åºåˆ—é•¿åº¦
        candidate_expand = candidate_embed.unsqueeze(1).expand(-1, seq_len, -1)
        
        # æ„é€ æ³¨æ„åŠ›è¾“å…¥ç‰¹å¾
        attention_input = torch.cat([
            candidate_expand,                           # å€™é€‰ç‰©å“ç‰¹å¾
            history_embeds,                            # å†å²ç‰©å“ç‰¹å¾  
            candidate_expand * history_embeds,         # å…ƒç´ çº§ä¹˜ç§¯
            candidate_expand - history_embeds          # å…ƒç´ çº§å·®å€¼
        ], dim=-1)
        
        # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
        attention_scores = self.attention_net(attention_input).squeeze(-1)
        
        # åº”ç”¨mask
        if mask is not None:
            attention_scores = attention_scores.masked_fill(~mask, -1e9)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.softmax(attention_scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        attended_embed = torch.sum(
            attention_weights.unsqueeze(-1) * history_embeds, dim=1
        )
        
        return attended_embed, attention_weights

class DIN(nn.Module):
    def __init__(self, item_vocab_size, embed_dim=64, hidden_dims=[256, 128]):
        super(DIN, self).__init__()
        
        # ç‰©å“åµŒå…¥å±‚
        self.item_embedding = nn.Embedding(item_vocab_size, embed_dim)
        
        # æ³¨æ„åŠ›å±‚
        self.attention = AttentionLayer(embed_dim)
        
        # æœ€ç»ˆé¢„æµ‹ç½‘ç»œ
        input_dim = embed_dim * 3  # ç”¨æˆ·ç‰¹å¾ + ç‰©å“ç‰¹å¾ + æ³¨æ„åŠ›ç‰¹å¾
        mlp_layers = []
        
        for i, dim in enumerate(hidden_dims):
            if i == 0:
                mlp_layers.extend([
                    nn.Linear(input_dim, dim),
                    nn.ReLU(),
                    nn.Dropout(0.2)
                ])
            else:
                mlp_layers.extend([
                    nn.Linear(hidden_dims[i-1], dim),
                    nn.ReLU(),
                    nn.Dropout(0.2) 
                ])
        
        mlp_layers.append(nn.Linear(hidden_dims[-1], 1))
        self.mlp = nn.Sequential(*mlp_layers)
        
    def forward(self, candidate_items, history_items, history_mask, user_features):
        # è·å–åµŒå…¥
        candidate_embeds = self.item_embedding(candidate_items)
        history_embeds = self.item_embedding(history_items)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attended_history, attention_weights = self.attention(
            candidate_embeds, history_embeds, history_mask
        )
        
        # ç‰¹å¾æ‹¼æ¥
        final_features = torch.cat([
            user_features,       # ç”¨æˆ·åŸºç¡€ç‰¹å¾
            candidate_embeds,    # å€™é€‰ç‰©å“ç‰¹å¾
            attended_history     # æ³¨æ„åŠ›åŠ æƒçš„å†å²ç‰¹å¾
        ], dim=-1)
        
        # æœ€ç»ˆé¢„æµ‹
        logits = self.mlp(final_features)
        return torch.sigmoid(logits), attention_weights
```

## è®­ç»ƒä¼˜åŒ–æŠ€å·§ ğŸ”§

### å¤šä»»åŠ¡å­¦ä¹ 

```python
class MultiTaskRecommender(nn.Module):
    def __init__(self, shared_dim=256, task_dims=[128, 64], input_dim=None):
        super(MultiTaskRecommender, self).__init__()
        
        # å…±äº«åº•å±‚ç½‘ç»œ
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, shared_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(shared_dim, shared_dim),
            nn.ReLU()
        )
        
        # ä»»åŠ¡ä¸“ç”¨ç½‘ç»œ
        self.ctr_tower = self._build_tower(shared_dim, task_dims)  # ç‚¹å‡»ç‡é¢„æµ‹
        self.cvr_tower = self._build_tower(shared_dim, task_dims)  # è½¬åŒ–ç‡é¢„æµ‹
        
    def _build_tower(self, input_dim, dims):
        layers = []
        for i, dim in enumerate(dims):
            if i == 0:
                layers.extend([nn.Linear(input_dim, dim), nn.ReLU()])
            else:
                layers.extend([nn.Linear(dims[i-1], dim), nn.ReLU()])
        layers.append(nn.Linear(dims[-1], 1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        shared_repr = self.shared_layers(x)
        
        ctr_logits = self.ctr_tower(shared_repr)
        cvr_logits = self.cvr_tower(shared_repr)
        
        return torch.sigmoid(ctr_logits), torch.sigmoid(cvr_logits)

# å¤šä»»åŠ¡æŸå¤±å‡½æ•°
def multi_task_loss(ctr_pred, cvr_pred, ctr_label, cvr_label, alpha=0.5):
    ctr_loss = nn.BCELoss()(ctr_pred, ctr_label)
    cvr_loss = nn.BCELoss()(cvr_pred, cvr_label)
    
    return alpha * ctr_loss + (1 - alpha) * cvr_loss
```

### é«˜çº§è®­ç»ƒç­–ç•¥

```python
# å­¦ä¹ ç‡è°ƒåº¦
def get_learning_rate_scheduler(optimizer, strategy='cosine'):
    if strategy == 'cosine':
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=100, eta_min=1e-6
        )
    elif strategy == 'warmup':
        def warmup_lambda(epoch):
            if epoch < 5:
                return epoch / 5.0
            else:
                return 0.95 ** (epoch - 5)
        return torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_lambda)

# æ¢¯åº¦ç´¯ç§¯
def train_with_gradient_accumulation(model, train_loader, accumulation_steps=4):
    optimizer = torch.optim.Adam(model.parameters())
    criterion = nn.BCELoss()
    
    optimizer.zero_grad()
    
    for i, (data, labels) in enumerate(train_loader):
        predictions = model(data)
        loss = criterion(predictions, labels) / accumulation_steps
        
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
```

## è¯„ä¼°ä¸éƒ¨ç½² ğŸ“Š

### è¯„ä¼°æŒ‡æ ‡

```python
def comprehensive_evaluation(y_true, y_pred, y_prob):
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
    
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc': roc_auc_score(y_true, y_prob)
    }
    
    return metrics

# A/Bæµ‹è¯•åˆ†æµ
def user_bucketing(user_id, experiment_ratio=0.1):
    """åŸºäºç”¨æˆ·IDçš„ä¸€è‡´æ€§åˆ†æµ"""
    import hashlib
    
    hash_value = int(hashlib.md5(str(user_id).encode()).hexdigest(), 16)
    bucket = hash_value % 100
    
    if bucket < experiment_ratio * 100:
        return 'experiment'
    else:
        return 'control'
```

## å®è·µæ€»ç»“ ğŸ¯

### æˆåŠŸç»éªŒ

1. **æ¨¡å‹é€‰æ‹©åŸåˆ™**ï¼š
   - æ•°æ®é‡å°ï¼šWide & Deep
   - ç‰¹å¾ä¸°å¯Œï¼šDeepFM  
   - åºåˆ—è¡Œä¸ºï¼šDIN

2. **å·¥ç¨‹ä¼˜åŒ–æŠ€å·§**ï¼š
   - åµŒå…¥å±‚åˆå§‹åŒ–ï¼šXavieræˆ–Heåˆå§‹åŒ–
   - æ‰¹è§„èŒƒåŒ–ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
   - æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

3. **ä¸šåŠ¡æŒ‡æ ‡å…³æ³¨**ï¼š
   - ä¸åªçœ‹AUCï¼Œæ›´è¦çœ‹ä¸šåŠ¡æ”¶ç›Š
   - å…³æ³¨é•¿æœŸç”¨æˆ·ä½“éªŒ
   - å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

### è°ƒä¼˜æŠ€å·§

1. **åµŒå…¥ç»´åº¦é€‰æ‹©**ï¼šé€šå¸¸ä¸º $\sqrt[4]{\text{vocab_size}} \times [10, 30]$
2. **å­¦ä¹ ç‡ç­–ç•¥**ï¼šembeddingå±‚ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼ŒDNNç”¨è¾ƒå¤§å­¦ä¹ ç‡
3. **æ­£åˆ™åŒ–ç»„åˆ**ï¼šL2 + Dropout + Batch Normalization

### æœªæ¥å‘å±•æ–¹å‘

1. **é¢„è®­ç»ƒæ¨¡å‹**ï¼šBERT4Rec, SASRecç­‰åºåˆ—æ¨èæ¨¡å‹
2. **å¤šæ¨¡æ€èåˆ**ï¼šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ä¿¡æ¯çš„æ•´åˆ
3. **å› æœæ¨æ–­**ï¼šå»åçš„æ¨èç®—æ³•
4. **è”é‚¦å­¦ä¹ **ï¼šéšç§ä¿æŠ¤çš„æ¨èç³»ç»Ÿ

> ğŸ’¡ **å…³é”®æ´å¯Ÿ**
> 
> æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿçš„æˆåŠŸä¸ä»…åœ¨äºå¤æ‚çš„ç½‘ç»œæ¶æ„ï¼Œæ›´åœ¨äºï¼š
> - å¯¹ä¸šåŠ¡åœºæ™¯çš„æ·±åˆ»ç†è§£
> - æ•°æ®è´¨é‡çš„ä¸¥æ ¼æŠŠæ§  
> - å·¥ç¨‹å®ç°çš„ç²¾ç›Šæ±‚ç²¾
> - æŒç»­è¿­ä»£çš„ä¼˜åŒ–ç²¾ç¥

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥æŒæ¡äº†ä¸»æµæ·±åº¦æ¨èç®—æ³•çš„åŸç†å’Œå®ç°ã€‚ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨EmbeddingæŠ€æœ¯â€”â€”æ·±åº¦æ¨èç³»ç»Ÿçš„åŸºçŸ³ã€‚ 